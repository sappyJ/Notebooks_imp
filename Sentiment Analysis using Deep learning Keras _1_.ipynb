{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# This notebok demonstrates how to use python NLTK package for text cleaning and text preparation. It also shows how to perform cosine similarity to find similar documents.\n\n## 2017 Dec Shilpa Jain", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Install Python NLTK package\n\nNLTK is a natural language toolkit for building programs in Python that work with natural language text.\nWe will use NLTK for this course.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!pip install nltk --upgrade", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting nltk\n  Downloading nltk-3.2.5.tar.gz (1.2MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.2MB 1.0MB/s eta 0:00:01\n\u001b[?25hCollecting six (from nltk)\n  Downloading six-1.11.0-py2.py3-none-any.whl\nBuilding wheels for collected packages: nltk\n  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /gpfs/fs01/user/s9ac-efa7ddefc45728-3c618564d05f/.cache/pip/wheels/18/9c/1f/276bc3f421614062468cb1c9d695e6086d0c73d67ea363c501\nSuccessfully built nltk\nInstalling collected packages: six, nltk\nSuccessfully installed nltk-3.2.5 six-1.11.0\n"
                }
            ], 
            "execution_count": 1
        }, 
        {
            "source": "## Import NLTK and download NLTK book collection", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import nltk\n\nnltk.download('punkt')\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[nltk_data] Downloading package punkt to /gpfs/fs01/user/s9ac-\n[nltk_data]     efa7ddefc45728-3c618564d05f/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "True"
                    }, 
                    "execution_count": 5, 
                    "metadata": {}
                }
            ], 
            "execution_count": 5
        }, 
        {
            "source": "## Cell below will load all the items in the book module that you have just downloaded. When this finishes, we will see the output.\nWe can see from the output that there are 9 pieces of text and 9 sentences loaded. For example, if we\ntype text1, we will see the title of the first piece of text. If we type sent3, we will see the body of the\nthird sentence.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\nimport sys\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share your notebook.\nclient_8760db995d144d1cab8bb99f8e30e4d7 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='Rx3KcqqyVM0CvH2mcoC-oNhWl4AIgJNnLG4yj6qWbLXG',\n    ibm_service_instance_id=\"iam-ServiceId-80092a7b-a6ab-4262-b7f0-11b4bbb372ac\",\n    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client_8760db995d144d1cab8bb99f8e30e4d7.get_object(Bucket='hdbc3bf4c3bf3454028bc34d6d765ab9a09',Key='HDB.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf_data_3 = pd.read_csv(body,encoding='utf-8')\ndf_data_train=df_data_3[:63]\ndf_data_test=df_data_3[64:]\ndf_data_train\ndf_data_test\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "    \ufeffCase                                               Data     Label\n64     65  I acknowledged that the Driver's information p...  Rejected\n65     66  I acknowledged that the Driver's information p...  Rejected\n66     67  I acknowledged that the Driver's information p...  Rejected\n67     68  I acknowledged that the Driver's information p...  Rejected\n68     69  I acknowledged that the Driver's information p...  Rejected\n69     70  I acknowledged that the Driver's information p...  Rejected\n70     71  I acknowledged that the Driver's information p...  Rejected\n71     72  I acknowledged that the Driver's information p...  Rejected\n72     73  I acknowledged that the Driver's information p...  Rejected\n73     74  I acknowledged that the Driver's information p...    Waived\n74     75  I acknowledged that the Driver's information p...    Waived\n75     76  I acknowledged that the Driver's information p...    Waived\n76     77  I acknowledged that the Driver's information p...    Waived\n77     78  I acknowledged that the Driver's information p...    Waived\n78     79  I acknowledged that the Driver's information p...    Waived\n79     80  I acknowledged that the Driver's information p...    Waived\n80     81  I acknowledged that the Driver's information p...    Waived\n81     82  I acknowledged that the Driver's information p...    Waived\n82     83  I,Ting Kok Choon I/c no:S1171455B is the drive...    Waived", 
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>\ufeffCase</th>\n      <th>Data</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>64</th>\n      <td>65</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Rejected</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>66</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Rejected</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>67</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Rejected</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>68</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Rejected</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>69</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Rejected</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>70</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Rejected</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>71</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Rejected</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>72</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Rejected</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>73</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Rejected</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>74</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Waived</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>75</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Waived</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>76</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Waived</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>77</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Waived</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>78</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Waived</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>79</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Waived</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>80</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Waived</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>81</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Waived</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>82</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Waived</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>83</td>\n      <td>I,Ting Kok Choon I/c no:S1171455B is the drive...</td>\n      <td>Waived</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    }, 
                    "execution_count": 2, 
                    "metadata": {}
                }
            ], 
            "execution_count": 2
        }, 
        {
            "source": "import nltk\ndocs_train=[]\ndocs_test=[]\nlabels_train=[]\nlabels_test=[]\nfor idx, row in df_data_train.iterrows():\n    #print (row['Label'])\n    \n    tokens = nltk.word_tokenize(row['Data'])\n    #print (nltk.pos_tag(tokens))\n    docs_train.append(row['Data'])\n    labels_train.append(row['Label'])\n    text2_train = nltk.Text(tokens)\n    #docs.append(tokens)\nprint (len(docs_train))\n\nfor idx, row in df_data_test.iterrows():\n    #print (row['Label'])\n    \n    tokens = nltk.word_tokenize(row['Data'])\n    docs_test.append(row['Data'])\n    labels_test.append(row['Label'])\n    text2_test = nltk.Text(tokens)\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "63\n"
                }
            ], 
            "execution_count": 72
        }, 
        {
            "source": "##### In NLTK, there is a method called concordance that allows us to search for a word inside a piece of text.\n##### Count method returns the number of times a word occurs in a piece of text.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## UsingWord Counts to Obtain an Overview of a Collection\nAssume that you have a large document collection. For example, it could be all the email enquiries\nfrom the customers of a company in a particular month. It could be all the tweets published by a particular\nuser. It could also be all the fictions written by a particular author. Without going through all the documents\ninside the collection, how can you quickly get an idea about the major topics or themes covered by these\ndocuments?\n\nIn NLTK, there is a built-in function called FreqDist() that makes our task very easy.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from nltk import *\nprint (text2_train)\nfdist=FreqDist(text2_train)\ntype(fdist)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "<Text: I acknowledged that the Driver 's information provided...>\n"
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "nltk.probability.FreqDist"
                    }, 
                    "execution_count": 4, 
                    "metadata": {}
                }
            ], 
            "execution_count": 4
        }, 
        {
            "source": "##### There is a method called most_common() that can be conveniently used to show the most frequent words in a frequency distribution.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "fdist.most_common(10)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[('.', 12),\n ('the', 9),\n (':', 9),\n ('to', 5),\n ('I', 5),\n ('No', 4),\n ('for', 3),\n ('and', 3),\n ('Name', 3),\n ('TOA', 2)]"
                    }, 
                    "execution_count": 5, 
                    "metadata": {}
                }
            ], 
            "execution_count": 5
        }, 
        {
            "source": "#### Looking at the most frequent words, you realize that they are not so meaningful. Many words are so commonly used everywhere that they do not reveal anything about the particular document or document collection we are looking at. There are a number of ways to address this problem.\n\n#### We will create a new list text2_long_words and add only words with atleast 5 characters.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!pip install gensim", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting gensim\n  Downloading gensim-3.2.0-cp35-cp35m-manylinux1_x86_64.whl (15.9MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15.9MB 81kB/s  eta 0:00:01  3% |\u2588\u258f                              | 573kB 35.1MB/s eta 0:00:01    19% |\u2588\u2588\u2588\u2588\u2588\u2588\u258e                         | 3.1MB 52.2MB/s eta 0:00:01    89% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 14.3MB 58.6MB/s eta 0:00:01\n\u001b[?25hCollecting smart-open>=1.2.1 (from gensim)\n  Downloading smart_open-1.5.6.tar.gz\nCollecting scipy>=0.18.1 (from gensim)\n  Downloading scipy-1.0.0-cp35-cp35m-manylinux1_x86_64.whl (49.6MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49.6MB 25kB/s  eta 0:00:01    20% |\u2588\u2588\u2588\u2588\u2588\u2588\u258d                         | 10.0MB 30.7MB/s eta 0:00:02    35% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                    | 17.8MB 24.5MB/s eta 0:00:02    46% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                 | 23.1MB 18.3MB/s eta 0:00:02\n\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): six>=1.5.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s9ac-efa7ddefc45728-3c618564d05f/.local/lib/python3.5/site-packages (from gensim)\nCollecting numpy>=1.11.3 (from gensim)\n  Downloading numpy-1.13.3-cp35-cp35m-manylinux1_x86_64.whl (16.9MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16.9MB 74kB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): boto>=2.32 in /usr/local/src/conda3_runtime.v25/4.1.1/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\nCollecting bz2file (from smart-open>=1.2.1->gensim)\n  Downloading bz2file-0.98.tar.gz\nRequirement already satisfied (use --upgrade to upgrade): requests in /usr/local/src/conda3_runtime.v25/4.1.1/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\nRequirement already satisfied (use --upgrade to upgrade): boto3 in /usr/local/src/conda3_runtime.v25/4.1.1/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\nRequirement already satisfied (use --upgrade to upgrade): botocore<1.6.0,>=1.5.0 in /usr/local/src/conda3_runtime.v25/4.1.1/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim)\nRequirement already satisfied (use --upgrade to upgrade): s3transfer<0.2.0,>=0.1.10 in /usr/local/src/conda3_runtime.v25/4.1.1/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim)\nRequirement already satisfied (use --upgrade to upgrade): jmespath<1.0.0,>=0.7.1 in /usr/local/src/conda3_runtime.v25/4.1.1/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim)\nRequirement already satisfied (use --upgrade to upgrade): python-dateutil<3.0.0,>=2.1 in /usr/local/src/conda3_runtime.v25/4.1.1/lib/python3.5/site-packages (from botocore<1.6.0,>=1.5.0->boto3->smart-open>=1.2.1->gensim)\nRequirement already satisfied (use --upgrade to upgrade): docutils>=0.10 in /usr/local/src/conda3_runtime.v25/4.1.1/lib/python3.5/site-packages (from botocore<1.6.0,>=1.5.0->boto3->smart-open>=1.2.1->gensim)\nBuilding wheels for collected packages: smart-open, bz2file\n  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /gpfs/fs01/user/s9ac-efa7ddefc45728-3c618564d05f/.cache/pip/wheels/36/48/35/97efc2bd1b233627131c9a936c9de23681846db707b907d353\n  Running setup.py bdist_wheel for bz2file ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /gpfs/fs01/user/s9ac-efa7ddefc45728-3c618564d05f/.cache/pip/wheels/31/9c/20/996d65ca104cbca940b1b053299b68459391c01c774d073126\nSuccessfully built smart-open bz2file\nInstalling collected packages: bz2file, smart-open, numpy, scipy, gensim\nSuccessfully installed bz2file-0.98 gensim-3.2.0 numpy-1.13.3 scipy-1.0.0 smart-open-1.5.6\n"
                }
            ], 
            "execution_count": 12
        }, 
        {
            "source": "from nltk.corpus import stopwords\nfrom nltk.stem.porter import *\nstemmer=PorterStemmer()\nimport gensim\nfrom gensim import corpora\nimport string\nfrom string import punctuation\nstoplist=['acknowledged','acknowledgement','driving','inform','Thanks','park','thank','postal','unit','appeal','driver','offence',\"Driver's\",'Driver','name','nric']\n\n# Input to dictionary is a list of list\n\n\n# turn a doc into clean tokens\ndef clean_doc(doc):\n\t# split into tokens by white space\n\ttokens = doc.split()\n\t# remove punctuation from each token\n\ttable = str.maketrans('', '', string.punctuation)\n\ttokens = [w.translate(table) for w in tokens]\n\t# remove remaining tokens that are not alphabetic\n\ttokens = [word for word in tokens if word.isalpha()]\n\t# filter out stop words\n\tstop_words = set(stopwords.words('english'))\n\ttokens = [w for w in tokens if not w in stop_words]\n\ttokens = [w.lower() for w in tokens]\n\ttokens = [w for w in tokens if not w in stoplist]\n\ttokens=[stemmer.stem(w) for w in tokens]\n\ttokens = [w for w in tokens if not w in stoplist]\n\t# filter out short tokens\n\ttokens = [word for word in tokens if len(word) > 4]\n\treturn tokens\n    \n    ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 6
        }, 
        {
            "source": "#nltk.download('stopwords')\nfrom collections import Counter\nvocab_train = Counter()\nvocab_test=Counter()\ndata_train=[]\ndata_test=[]\nfor i in docs_train:\n    tokens=clean_doc(i)\n    data_train.append(tokens)\n    vocab_train.update(tokens)\nprint(len(vocab_train))    \nfor i in docs_test:\n    tokens=clean_doc(i)\n    data_test.append(tokens)\n    vocab_test.update(tokens)\nprint(len(vocab_test))\n# print the top words in the vocab\nprint(vocab_train.most_common(50))\nm=vocab_train.most_common(50)\ndata_train\n#print (dictionary)\n#print (vocab)\n#print (content)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "728\n362\n[('level', 58), ('provid', 58), ('build', 57), ('explan', 57), ('blkhous', 57), ('nopassport', 57), ('street', 57), ('identif', 57), ('vehicl', 41), ('carpark', 38), ('season', 38), ('summon', 36), ('notic', 29), ('could', 26), ('would', 26), ('understand', 23), ('payoh', 20), ('pleas', 15), ('unload', 15), ('deliveri', 14), ('ticket', 14), ('realli', 13), ('sincer', 13), ('offic', 13), ('first', 13), ('howev', 11), ('place', 11), ('ensur', 11), ('minut', 11), ('toilet', 11), ('receiv', 11), ('number', 11), ('write', 10), ('avail', 10), ('block', 10), ('nearbi', 10), ('lenienc', 10), ('waiver', 10), ('appreci', 9), ('visit', 9), ('intent', 9), ('punggol', 9), ('short', 9), ('matter', 9), ('holder', 9), ('reason', 8), ('sorri', 8), ('normal', 8), ('urgent', 8), ('obstruct', 8)]\n"
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[['provid',\n  'yinghan',\n  'edwin',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'payoh',\n  'level',\n  'build',\n  'payoh',\n  'explan',\n  'dirti',\n  'instruct',\n  'helper',\n  'condit',\n  'attend',\n  'summon',\n  'clearli',\n  'peopl',\n  'charg',\n  'barrier',\n  'wrongli',\n  'engin',\n  'still',\n  'drove',\n  'distanc',\n  'clearli',\n  'check',\n  'pleas',\n  'expedit',\n  'chang'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'tampin',\n  'level',\n  'build',\n  'tampin',\n  'spring',\n  'explan',\n  'diahorrea',\n  'empti',\n  'although',\n  'stomach',\n  'realli',\n  'record',\n  'clean',\n  'kindli',\n  'chanc'],\n ['provid',\n  'liang',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'marsil',\n  'level',\n  'build',\n  'marsil',\n  'garden',\n  'explan',\n  'offic',\n  'charg',\n  'write',\n  'plead',\n  'lenienc',\n  'follow',\n  'offens',\n  'notic',\n  'number',\n  'appoint',\n  'friend',\n  'wheelchairbound',\n  'grandmoth',\n  'attend',\n  'howev',\n  'necessari',\n  'fulfil',\n  'requir',\n  'handicap',\n  'therefor',\n  'grace',\n  'second',\n  'chanc',\n  'toward',\n  'matter',\n  'consider',\n  'liang'],\n ['provid',\n  'mohamad',\n  'hakim',\n  'ibrahim',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'woodland',\n  'level',\n  'build',\n  'woodland',\n  'explan',\n  'toilet',\n  'urgent',\n  'there',\n  'avail',\n  'season',\n  'nearbi',\n  'summon',\n  'could'],\n ['provid',\n  'irwan',\n  'ghazali',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'woodland',\n  'level',\n  'build',\n  'explan',\n  'mention',\n  'deliv',\n  'stool',\n  'pictur',\n  'attach',\n  'nearbi',\n  'block',\n  'along',\n  'servic',\n  'doubl',\n  'indic',\n  'block',\n  'obstruct',\n  'switch',\n  'hazard',\n  'light',\n  'unload',\n  'nearest',\n  'block',\n  'bulki',\n  'vehicl',\n  'multistorey',\n  'carpark',\n  'distanc',\n  'intent',\n  'whatsoev',\n  'vehicl',\n  'illeg',\n  'gantri',\n  'enter',\n  'carpark',\n  'regret',\n  'action',\n  'ensur',\n  'would',\n  'repeat',\n  'pleas',\n  'summon',\n  'sorri',\n  'inconvenienc'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'serangoon',\n  'level',\n  'build',\n  'explan',\n  'sirmadam',\n  'still',\n  'recov',\n  'problem',\n  'understand',\n  'excus',\n  'prove',\n  'genuin',\n  'excus',\n  'special',\n  'govern',\n  'polyclin',\n  'excus',\n  'understand',\n  'sometim',\n  'realli',\n  'downstair',\n  'short',\n  'multistori',\n  'carpark',\n  'peopl',\n  'problem',\n  'realli',\n  'notic',\n  'excus',\n  'contact',\n  'phone',\n  'front',\n  'windscreen',\n  'realli',\n  'obstruct',\n  'deliveri',\n  'season',\n  'ticket',\n  'holder',\n  'serangoon',\n  'north',\n  'reason',\n  'stupid',\n  'openli',\n  'downstair',\n  'block',\n  'shelter',\n  'carpark',\n  'understand',\n  'attach',\n  'excus',\n  'photo'],\n ['provid',\n  'hyong',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'hougang',\n  'level',\n  'build',\n  'explan',\n  'write',\n  'waiver',\n  'summon',\n  'deliveri',\n  'custom',\n  'unexpectedli',\n  'receiv',\n  'summon',\n  'sincer',\n  'could',\n  'summon',\n  'attent',\n  'receiv',\n  'favour',\n  'repli'],\n ['provid',\n  'tianwen',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'level',\n  'build',\n  'explan',\n  'drive',\n  'bring',\n  'hospit',\n  'fever',\n  'friend',\n  'someth',\n  'thing',\n  'around',\n  'minut',\n  'fever',\n  'friend',\n  'distanc',\n  'public',\n  'sorri',\n  'purpos',\n  'forgiv'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'level',\n  'build',\n  'height',\n  'explan',\n  'offic',\n  'charg',\n  'deliveri',\n  'serangoon',\n  'north',\n  'wesley',\n  'child',\n  'develop',\n  'centr',\n  'carpark',\n  'attend',\n  'stand',\n  'serangoon',\n  'north',\n  'polit',\n  'deliveri',\n  'grace',\n  'period',\n  'problem',\n  'carri',\n  'deliveri',\n  'lorri',\n  'found',\n  'summon',\n  'ticket',\n  'carpark',\n  'attend'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'punggol',\n  'level',\n  'build',\n  'punggol',\n  'emerald',\n  'explan',\n  'sirmadam',\n  'write',\n  'notic',\n  'refer',\n  'number',\n  'summon',\n  'unauthor',\n  'regul',\n  'attempt',\n  'visitor',\n  'terribl',\n  'hurri',\n  'bring',\n  'child',\n  'public',\n  'toilet',\n  'uncharacterist',\n  'reluct',\n  'decis',\n  'temporarili',\n  'avail',\n  'howev',\n  'consciou',\n  'effort',\n  'obstruct',\n  'quick',\n  'action',\n  'bring',\n  'child',\n  'minut',\n  'acced',\n  'request',\n  'waiver',\n  'urgenc',\n  'natur',\n  'understand',\n  'import',\n  'ensur',\n  'pleasant',\n  'journey',\n  'commut',\n  'utmost',\n  'ensur',\n  'futur',\n  'sincer'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'punggol',\n  'level',\n  'build',\n  'waterway',\n  'woodcress',\n  'explan',\n  'sirmdm',\n  'write',\n  'notic',\n  'refer',\n  'number',\n  'father',\n  'visit',\n  'father',\n  'agent',\n  'father',\n  'becaus',\n  'wanter',\n  'vehicl',\n  'temporarili',\n  'opposit',\n  'father',\n  'today',\n  'agreement',\n  'acced',\n  'request',\n  'waiver'],\n ['provid',\n  'desmond',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'level',\n  'build',\n  'explan',\n  'stall',\n  'stall',\n  'number',\n  'follow',\n  'vista',\n  'market',\n  'sincer',\n  'apologis',\n  'inconvenienc'],\n ['provid',\n  'desmond',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'level',\n  'build',\n  'explan',\n  'stall',\n  'stall',\n  'number',\n  'follow',\n  'vista',\n  'market',\n  'sincer',\n  'apologis',\n  'inconvenienc'],\n ['provid',\n  'muhammad',\n  'khairulanwar',\n  'abdul',\n  'karim',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'level',\n  'build',\n  'kebun',\n  'height',\n  'explan',\n  'firstli',\n  'appologis',\n  'sunday',\n  'visitor',\n  'fulli',\n  'occupi',\n  'carpark',\n  'urgent',\n  'toilet',\n  'stomach',\n  'upset',\n  'particular',\n  'choic',\n  'season',\n  'durat',\n  'secondli',\n  'carpark',\n  'didnt',\n  'receiv',\n  'summon',\n  'coupon',\n  'offic',\n  'owner',\n  'vehicl',\n  'receiv',\n  'letter',\n  'would',\n  'kindli',\n  'first',\n  'happen',\n  'urgent',\n  'lookong',\n  'toilet',\n  'point',\n  'pleas',\n  'assist',\n  'matter',\n  'ensur',\n  'matter',\n  'repeat',\n  'sincer',\n  'khairul'],\n ['provid',\n  'yanjun',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'level',\n  'build',\n  'grove',\n  'explan',\n  'refer',\n  'notic',\n  'season',\n  'purchas',\n  'summon',\n  'still',\n  'given',\n  'regard'],\n ['provid',\n  'jumaat',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'punggol',\n  'level',\n  'build',\n  'punggol',\n  'crest',\n  'explan',\n  'jumaat',\n  'furnitur',\n  'aunti',\n  'vehicl',\n  'summon',\n  'ticket',\n  'sincer',\n  'appreci',\n  'could',\n  'realli',\n  'vehicl'],\n ['provid',\n  'jacki',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'sengkang',\n  'level',\n  'build',\n  'compassval',\n  'ancilla',\n  'explan',\n  'offic',\n  'incharg',\n  'brought',\n  'grandma',\n  'wheel',\n  'chair',\n  'dinner',\n  'canteen',\n  'howev',\n  'first',\n  'infront',\n  'canteen',\n  'limit',\n  'drive',\n  'realis',\n  'canteen',\n  'heavi',\n  'vehicl',\n  'decid',\n  'temporarili',\n  'grandma',\n  'canteen',\n  'spaciou',\n  'block',\n  'congesst',\n  'actual',\n  'return',\n  'alreadi',\n  'could',\n  'review',\n  'chanc',\n  'never',\n  'ticket',\n  'place',\n  'never',\n  'small',\n  'blind',\n  'lorri',\n  'inconveni',\n  'lorri',\n  'might',\n  'damag',\n  'chanc',\n  'assur',\n  'happen'],\n ['provid',\n  'muham',\n  'bahrudeen',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'level',\n  'build',\n  'explan',\n  'thiam',\n  'gener',\n  'manag',\n  'branch',\n  'would',\n  'notic',\n  'notic',\n  'receiv',\n  'notic',\n  'servic',\n  'block',\n  'avenu',\n  'infring',\n  'vehicl',\n  'number',\n  'arriv',\n  'locat',\n  'carri',\n  'child',\n  'school',\n  'groceri',\n  'bring',\n  'thing',\n  'understand',\n  'servic',\n  'wrong',\n  'heavi',\n  'thing',\n  'bring',\n  'child',\n  'minut',\n  'unload',\n  'start',\n  'vehicl',\n  'warden',\n  'moham',\n  'apandi',\n  'stand',\n  'right',\n  'front',\n  'vehicl',\n  'obstruct',\n  'notic',\n  'without',\n  'pictur',\n  'vehicl',\n  'locat',\n  'patient',\n  'wound',\n  'screen',\n  'could',\n  'instead',\n  'block',\n  'vehicl',\n  'summon',\n  'repli',\n  'branch',\n  'immedi',\n  'carpark',\n  'hotlin',\n  'later',\n  'warden',\n  'moham',\n  'apandi',\n  'black',\n  'jacket',\n  'conceal',\n  'warden',\n  'uniform',\n  'other',\n  'vehicl',\n  'vehicl',\n  'remov',\n  'black',\n  'jacket',\n  'start',\n  'summon',\n  'thrice',\n  'instead',\n  'vehicl',\n  'intent',\n  'ambush',\n  'vehicl',\n  'summon',\n  'understand',\n  'servic',\n  'allow',\n  'illeg',\n  'action',\n  'could',\n  'taken',\n  'sometim',\n  'flexibl',\n  'could',\n  'exercis',\n  'genuin',\n  'unload',\n  'understand',\n  'warden',\n  'everi',\n  'right',\n  'summon',\n  'though',\n  'vehicl',\n  'action',\n  'courtesi',\n  'rater',\n  'block',\n  'vehicl',\n  'block',\n  'summon',\n  'uncal',\n  'disobey',\n  'instruct',\n  'scold'],\n ['sirmdm',\n  'reappeal',\n  'notic',\n  'requir',\n  'visit',\n  'dealer',\n  'howev',\n  'nonep',\n  'usual',\n  'conveni',\n  'quick',\n  'short',\n  'deliveri',\n  'collect',\n  'paymentetc',\n  'coupon',\n  'claim',\n  'compani',\n  'conveni',\n  'think',\n  'usual',\n  'furthermor',\n  'grace',\n  'period',\n  'pocket',\n  'understand',\n  'anoth',\n  'matter',\n  'total',\n  'unrel',\n  'pleas',\n  'reject',\n  'reason',\n  'alreadi',\n  'pleas',\n  'reconsid',\n  'exercis',\n  'certain',\n  'level',\n  'lenienc',\n  'display',\n  'coupon',\n  'visit',\n  'onward',\n  'realli',\n  'drive',\n  'increas',\n  'charg',\n  'petrol',\n  'occasion',\n  'summon',\n  'pleas'],\n ['provid',\n  'xinshengedwin',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'level',\n  'build',\n  'explan',\n  'grandma',\n  'visit',\n  'newli',\n  'purchas',\n  'happi',\n  'difficult',\n  'downstair',\n  'unload',\n  'technic',\n  'unload',\n  'wheelchair',\n  'pleas'],\n ['provid',\n  'cheng',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'punggol',\n  'level',\n  'build',\n  'sundial',\n  'explan',\n  'herebi',\n  'would',\n  'mention',\n  'deliveri',\n  'locat',\n  'white',\n  'unfortun',\n  'carss',\n  'owner',\n  'usual',\n  'deliveri',\n  'around',\n  'health',\n  'condit',\n  'season',\n  'could',\n  'deliveri',\n  'return',\n  'short',\n  'drive',\n  'vehicl',\n  'could',\n  'unfortun',\n  'receiv',\n  'could',\n  'check',\n  'carpark',\n  'record',\n  'realli',\n  'short',\n  'heart',\n  'bypass',\n  'surgeri',\n  'admit',\n  'hospit',\n  'heart',\n  'discomfort',\n  'appreci',\n  'could',\n  'health',\n  'condit',\n  'consider'],\n ['provid',\n  'eugen',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'flora',\n  'drive',\n  'level',\n  'build',\n  'dahlia',\n  'condominium',\n  'explan',\n  'would',\n  'mention',\n  'summon',\n  'receiv',\n  'carpark',\n  'notic',\n  'portion',\n  'white',\n  'colour',\n  'complet',\n  'white',\n  'intent',\n  'white',\n  'proceed',\n  'place',\n  'genuin',\n  'meant',\n  'season',\n  'thought',\n  'instruct',\n  'display',\n  'entranceexit',\n  'carpark',\n  'meant',\n  'white',\n  'would',\n  'understand',\n  'intent',\n  'charg',\n  'would',\n  'season',\n  'appli',\n  'whole',\n  'carpark',\n  'favour',\n  'repli',\n  'pleas',\n  'contact',\n  'queri',\n  'requir',\n  'clarif'],\n ['sirmdm',\n  'season',\n  'ticket',\n  'holder',\n  'carpark',\n  'season',\n  'normal',\n  'season',\n  'holder',\n  'lorriesvan',\n  'unload',\n  'council',\n  'urgent',\n  'import',\n  'document',\n  'mother',\n  'choic',\n  'within',\n  'furthermor',\n  'would',\n  'request',\n  'waiver',\n  'would',\n  'consid',\n  'contact',\n  'regard',\n  'melvin'],\n ['provid',\n  'raduan',\n  'ramli',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'compassval',\n  'level',\n  'build',\n  'explan',\n  'first',\n  'passeng',\n  'realis',\n  'intent'],\n ['summon',\n  'approach',\n  'summon',\n  'famili',\n  'financi',\n  'hardship',\n  'result',\n  'accumul',\n  'money',\n  'whole',\n  'month',\n  'season',\n  'arrear',\n  'vehicl',\n  'incom',\n  'famili',\n  'accumul',\n  'afford',\n  'season',\n  'sincerli',\n  'request',\n  'waiver',\n  'summon',\n  'pleas'],\n ['provid',\n  'kishor',\n  'balakrisnan',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'bedok',\n  'level',\n  'build',\n  'explan',\n  'offic',\n  'charg',\n  'sirmdm',\n  'kindli',\n  'kishor',\n  'balakrisnan',\n  'basement',\n  'tekka',\n  'centr',\n  'buffalo',\n  'heavili',\n  'enter',\n  'mention',\n  'state',\n  'notic',\n  'close',\n  'renov',\n  'frankli',\n  'could',\n  'normal',\n  'white',\n  'fulli',\n  'occupi',\n  'heavili',\n  'normal',\n  'avail',\n  'choic',\n  'season',\n  'return',\n  'summon',\n  'offencei',\n  'write',\n  'enquiri',\n  'attend',\n  'sirmdm',\n  'kindli',\n  'circumst',\n  'action',\n  'lenient',\n  'renov',\n  'sincer',\n  'nomal',\n  'avail',\n  'state',\n  'hurriedli',\n  'season',\n  'sincer',\n  'circumst',\n  'renov',\n  'period',\n  'timei',\n  'attach',\n  'pictur',\n  'renov',\n  'futur',\n  'vigil',\n  'vehicl',\n  'public'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'lorong',\n  'payoh',\n  'level',\n  'build',\n  'oleand',\n  'tower',\n  'explan',\n  'could',\n  'despit',\n  'circl',\n  'develop',\n  'strong',\n  'toilet',\n  'could',\n  'longer',\n  'fortun',\n  'drove',\n  'thought',\n  'quickli',\n  'defec',\n  'notic',\n  'motorcycl',\n  'previou',\n  'illeg',\n  'behind',\n  'payoh',\n  'central',\n  'immedi',\n  'nearbi',\n  'coffeeshop',\n  'washroom',\n  'after',\n  'reliev',\n  'quickli',\n  'pickup',\n  'essenti',\n  'minut',\n  'later',\n  'found',\n  'notic',\n  'realis',\n  'happen',\n  'person',\n  'emerg',\n  'extrem',\n  'urgent',\n  'situat',\n  'compass',\n  'understand',\n  'author',\n  'intent',\n  'furthermor',\n  'sunday',\n  'knowingli',\n  'commit',\n  'incur'],\n ['provid',\n  'kanalita',\n  'ahmad',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'bedok',\n  'level',\n  'build',\n  'explan',\n  'sirmadam',\n  'notic',\n  'number',\n  'waiver',\n  'night',\n  'grand',\n  'daughter',\n  'toilet',\n  'urgenc',\n  'nearbi',\n  'canteen',\n  'moment',\n  'notic',\n  'season',\n  'compass',\n  'lenienc',\n  'matter',\n  'favour',\n  'repli',\n  'sincer',\n  'kanalita'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'fernval',\n  'level',\n  'build',\n  'fernval',\n  'explan',\n  'sorri',\n  'season',\n  'becaus',\n  'quick',\n  'breakfast',\n  'plenti',\n  'empti',\n  'pleas'],\n ['provid',\n  'thavamani',\n  'pugalendhi',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'siang',\n  'kuang',\n  'avenu',\n  'level',\n  'build',\n  'sennett',\n  'estat',\n  'explan',\n  'sirmadam',\n  'refer',\n  'mention',\n  'notic',\n  'vehicl',\n  'season',\n  'place',\n  'emerg',\n  'staff',\n  'nearbi',\n  'immedi',\n  'vehicl',\n  'minut',\n  'herebi',\n  'assur',\n  'offenc',\n  'repeat',\n  'futur',\n  'strict',\n  'instruct',\n  'follow',\n  'point',\n  'timekindli',\n  'consid',\n  'composit',\n  'money',\n  'regard',\n  'sundaram',\n  'murugan'],\n ['provid',\n  'chang',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'level',\n  'build',\n  'explan',\n  'around',\n  'redwhit',\n  'appoint',\n  'nearbi',\n  'couldnt',\n  'white',\n  'avail',\n  'carpark',\n  'around',\n  'finish',\n  'appoint',\n  'nearbi',\n  'short',\n  'period',\n  'understand',\n  'notic',\n  'valid',\n  'redwhit',\n  'becom',\n  'season',\n  'realli',\n  'unabl',\n  'valid',\n  'import',\n  'appoint',\n  'involv',\n  'import',\n  'commer',\n  'reflect',\n  'realis',\n  'inconveni',\n  'local',\n  'resid',\n  'season',\n  'sorri',\n  'mistak',\n  'consid',\n  'repeat',\n  'error',\n  'humbl',\n  'consider'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'level',\n  'build',\n  'explan',\n  'mention',\n  'order',\n  'hawker',\n  'centr',\n  'collect',\n  'receiv',\n  'readi',\n  'collect',\n  'drove',\n  'collect',\n  'experienc',\n  'stomach',\n  'couldnt',\n  'avail',\n  'reach',\n  'minut',\n  'season',\n  'beyond',\n  'could',\n  'endur',\n  'toilet',\n  'hawker',\n  'centr',\n  'reliev',\n  'carpark',\n  'first',\n  'thing',\n  'instead',\n  'collect',\n  'normal',\n  'could',\n  'collect',\n  'later',\n  'surpris',\n  'warden',\n  'summon',\n  'check',\n  'plead',\n  'lenienc',\n  'explain',\n  'predica',\n  'repli',\n  'without',\n  'understand',\n  'could',\n  'explain',\n  'begin',\n  'ticket',\n  'thing',\n  'could',\n  'instead',\n  'promptli',\n  'remov',\n  'vehicl',\n  'normal',\n  'frequent',\n  'carpark',\n  'weekend',\n  'alway',\n  'normal',\n  'howev',\n  'unbear',\n  'stomach',\n  'upset',\n  'reliev',\n  'toilet',\n  'understand',\n  'committe',\n  'plead',\n  'first',\n  'recurr'],\n ['provid',\n  'cherng',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'jalan',\n  'level',\n  'build',\n  'eastern',\n  'garden',\n  'explan',\n  'offic',\n  'waiver',\n  'summon',\n  'number',\n  'medic',\n  'compassion',\n  'reason',\n  'urgent',\n  'thomson',\n  'medic',\n  'centr',\n  'complic',\n  'major',\n  'surgeri',\n  'almost',\n  'imposs',\n  'choic',\n  'season',\n  'futur',\n  'peopl',\n  'visit',\n  'urgent',\n  'matter',\n  'hospit',\n  'hybrid',\n  'season',\n  'would',\n  'appreci',\n  'kindli',\n  'review',\n  'summon',\n  'pleas',\n  'definit',\n  'choic',\n  'desper'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'littl',\n  'level',\n  'build',\n  'explan',\n  'regard',\n  'vicin',\n  'unload',\n  'choic',\n  'deliveri',\n  'truck',\n  'ensur',\n  'obstruct',\n  'traffic',\n  'complet',\n  'deliveri',\n  'within',\n  'minut',\n  'lenienc',\n  'summon',\n  'attach',\n  'proof',\n  'deliveri',\n  'invoic'],\n ['provid',\n  'logeswaran',\n  'anpalagan',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'bedok',\n  'reservoir',\n  'level',\n  'build',\n  'rainbow',\n  'explan',\n  'didnt',\n  'notic',\n  'carpark',\n  'reserv',\n  'season',\n  'holder',\n  'pleas',\n  'reduc',\n  'amount',\n  'first',\n  'offens'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'fernval',\n  'level',\n  'build',\n  'fernval',\n  'explan',\n  'offic',\n  'incharg',\n  'offic',\n  'write',\n  'notic',\n  'vehicl',\n  'friend',\n  'shift',\n  'bulki',\n  'elder',\n  'grandpar',\n  'unload',\n  'therefor',\n  'besid',\n  'roadsid',\n  'nearest',\n  'slope',\n  'toward',\n  'lobbi',\n  'newli',\n  'purchas',\n  'settl',\n  'grand',\n  'mother',\n  'payoh',\n  'lorong',\n  'unload',\n  'place',\n  'elder',\n  'grand',\n  'mother',\n  'place',\n  'sorri',\n  'unwant',\n  'inconveni',\n  'resid',\n  'nearbi',\n  'process',\n  'offic',\n  'incharg',\n  'would',\n  'consid',\n  'waiver',\n  'owner'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'woodland',\n  'level',\n  'build',\n  'explan',\n  'sirmadam',\n  'abovement',\n  'summon',\n  'receiv',\n  'vehicl',\n  'follow',\n  'ground',\n  'speciallabel',\n  'ticket',\n  'holder',\n  'bishan',\n  'afternoon',\n  'suddenli',\n  'stomachach',\n  'hurri',\n  'carpark',\n  'bishan',\n  'toilet',\n  'short',\n  'although',\n  'speial',\n  'label',\n  'ticket',\n  'holder',\n  'entitl',\n  'bishan',\n  'carpark',\n  'season',\n  'ticket',\n  'holder',\n  'sorri',\n  'mistak',\n  'first',\n  'receiv',\n  'summon',\n  'natur',\n  'hurriedli',\n  'nearest',\n  'avail',\n  'would',\n  'kindli',\n  'forgiv',\n  'mistak',\n  'summon',\n  'favoral',\n  'repli'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'level',\n  'build',\n  'explan',\n  'season',\n  'holder',\n  'place',\n  'carpark',\n  'upstair',\n  'drizzl',\n  'subsequ',\n  'downstair',\n  'receiv',\n  'offens',\n  'pleas',\n  'would',\n  'lenienc'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'level',\n  'build',\n  'explan',\n  'vehicl',\n  'space',\n  'carpark',\n  'summon',\n  'notic',\n  'without',\n  'valid',\n  'season',\n  'ticket',\n  'recent',\n  'transfer',\n  'vehicl',\n  'number',\n  'possibl',\n  'system',\n  'updat',\n  'assist',\n  'matter',\n  'understand',\n  'summon',\n  'notic'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'payoh',\n  'level',\n  'build',\n  'central',\n  'horizon',\n  'explan',\n  'write',\n  'august',\n  'neighbourhood',\n  'payoh',\n  'shift',\n  'belong',\n  'place',\n  'payoh',\n  'unload',\n  'grant',\n  'access',\n  'intent',\n  'obstruct',\n  'traffic',\n  'heavi',\n  'object',\n  'luggag',\n  'would',\n  'extrem',\n  'difficult',\n  'carri',\n  'stair',\n  'carpark',\n  'understand',\n  'situat',\n  'pleas',\n  'transport',\n  'necessari',\n  'shift',\n  'intent',\n  'inconveni',\n  'anyon',\n  'understand',\n  'would',\n  'appreci',\n  'would',\n  'acced',\n  'request',\n  'happi',\n  'nation'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'payoh',\n  'level',\n  'build',\n  'explan',\n  'madam',\n  'season',\n  'overdu',\n  'payment',\n  'would',\n  'current',\n  'overdu',\n  'payment',\n  'oversea',\n  'unabl',\n  'perform',\n  'onlin',\n  'transact',\n  'sorri',\n  'arrang',\n  'payment',\n  'earlier',\n  'consider',\n  'sincer'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'payoh',\n  'level',\n  'build',\n  'payoh',\n  'court',\n  'explan',\n  'drive',\n  'sickli',\n  'handicap',\n  'mother',\n  'shift',\n  'vehicl',\n  'carpark',\n  'normal',\n  'immediatelyit',\n  'check',\n  'household',\n  'mother'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'payoh',\n  'central',\n  'level',\n  'build',\n  'payoh',\n  'central',\n  'commun',\n  'explan',\n  'sirmdm',\n  'write',\n  'notic',\n  'vehicl',\n  'unmark',\n  'polic',\n  'vehicl',\n  'belong',\n  'payoh',\n  'neighbourhood',\n  'polic',\n  'centr',\n  'unmark',\n  'polic',\n  'vehicl',\n  'season',\n  'attend',\n  'unabl',\n  'avail',\n  'would',\n  'notic',\n  'apolog',\n  'inconveni',\n  'causedi',\n  'contact',\n  'clarif'],\n ['provid',\n  'victor',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'bright',\n  'drive',\n  'level',\n  'build',\n  'thomson',\n  'condominium',\n  'explan',\n  'grandson',\n  'usual',\n  'longer',\n  'expect',\n  'parent',\n  'slightli',\n  'later',\n  'usual',\n  'ensur',\n  'taken',\n  'ensur',\n  'happen'],\n ['provid',\n  'loong',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'changi',\n  'level',\n  'build',\n  'explan',\n  'sirmdm',\n  'write',\n  'notic',\n  'refer',\n  'number',\n  'visit',\n  'thomson',\n  'medic',\n  'center',\n  'check',\n  'howev',\n  'queue',\n  'decid',\n  'carpark',\n  'nearbi',\n  'close',\n  'minut',\n  'carpark',\n  'howev',\n  'still',\n  'avail',\n  'alight',\n  'registr',\n  'first',\n  'commenc',\n  'unabl',\n  'longer',\n  'queue',\n  'terribl',\n  'hurri',\n  'uncharacterist',\n  'reluct',\n  'decis',\n  'temporarili',\n  'first',\n  'child',\n  'opportun',\n  'acced',\n  'request',\n  'waiver',\n  'unfortun',\n  'event',\n  'start',\n  'famili',\n  'realli',\n  'financi',\n  'understand',\n  'import',\n  'ensur',\n  'pleasant',\n  'journey',\n  'commut',\n  'utmost',\n  'ensur',\n  'futur',\n  'understand',\n  'truli',\n  'sorri',\n  'inconveni',\n  'regard'],\n ['provid',\n  'shirlyn',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'payoh',\n  'level',\n  'build',\n  'payoh',\n  'explan',\n  'luggag',\n  'hubbi',\n  'flight',\n  'thought',\n  'alreadi',\n  'short',\n  'howev',\n  'daughter',\n  'tummi',\n  'delay',\n  'consider'],\n ['provid',\n  'muhammad',\n  'jumaad',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'simei',\n  'level',\n  'build',\n  'explan',\n  'sirmaam',\n  'muhammad',\n  'would',\n  'request',\n  'summon',\n  'hurri',\n  'empti',\n  'bowel',\n  'frequent',\n  'visit',\n  'toilet',\n  'poison',\n  'seriou',\n  'visit',\n  'consult',\n  'session',\n  'forgoten',\n  'coupon',\n  'short',\n  'continu',\n  'coupon',\n  'despair',\n  'summon',\n  'alreadi',\n  'summon',\n  'offic',\n  'pleas',\n  'forgiv',\n  'purpos',\n  'bowel',\n  'problem',\n  'appreci',\n  'could',\n  'consider',\n  'matter',\n  'appreci',\n  'could',\n  'merci'],\n ['provid',\n  'samantha',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'level',\n  'build',\n  'central',\n  'explan',\n  'stomachach',\n  'diahrrea',\n  'vomit',\n  'myalgia',\n  'fever',\n  'celsiu',\n  'month',\n  'daughter',\n  'symptom',\n  'throughout',\n  'night',\n  'sleep',\n  'fatigu',\n  'place',\n  'could',\n  'return',\n  'could',\n  'could',\n  'without',\n  'effort',\n  'requir',\n  'place',\n  'howev',\n  'white',\n  'occupi',\n  'decid',\n  'short',\n  'season',\n  'total',\n  'otherwis',\n  'would',\n  'anoth',\n  'carpark',\n  'distanc',\n  'total',\n  'carri',\n  'everi',\n  'challeng',\n  'sincer',\n  'understand',\n  'empathi',\n  'condit',\n  'acknowledg',\n  'broken',\n  'appreci',\n  'could',\n  'grant',\n  'waiver',\n  'composit',\n  'consid',\n  'carri',\n  'attach',\n  'daughter',\n  'birth',\n  'daughter',\n  'doctor',\n  'consult',\n  'photo',\n  'fever',\n  'temperatur',\n  'support',\n  'document',\n  'situat',\n  'could',\n  'review',\n  'grant',\n  'lenienc',\n  'situat'],\n ['write', 'impos', 'serangoon', 'north', 'avenu'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'serangoon',\n  'level',\n  'build',\n  'explan',\n  'would',\n  'lenienc',\n  'intent',\n  'season',\n  'season',\n  'would',\n  'plead',\n  'composit',\n  'regard',\n  'kelvin'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'level',\n  'build',\n  'parkview',\n  'explan',\n  'concern',\n  'deliveri',\n  'friend',\n  'hawker',\n  'stall',\n  'loadingunload',\n  'behind',\n  'squar',\n  'around',\n  'deliv',\n  'given',\n  'ticket',\n  'check',\n  'around',\n  'realiz',\n  'commerci',\n  'vehicl',\n  'given',\n  'puzzl',\n  'vehicl',\n  'given',\n  'ticket',\n  'report',\n  'offic',\n  'didnt',\n  'realiz',\n  'vehicl',\n  'although',\n  'clearli',\n  'plate',\n  'perhap',\n  'didnt',\n  'realis',\n  'first',\n  'place',\n  'consid',\n  'perceiev',\n  'color',\n  'model',\n  'understand',\n  'allow',\n  'normal',\n  'deliveri',\n  'unload',\n  'vehicl',\n  'believ',\n  'valid',\n  'reason',\n  'temporari',\n  'unload',\n  'right',\n  'clearli',\n  'violat',\n  'sincer'],\n ['refer',\n  'mention',\n  'notic',\n  'resid',\n  'serangoon',\n  'north',\n  'kindli',\n  'implor',\n  'consider',\n  'mention',\n  'notic',\n  'offic',\n  'moment',\n  'anxieti',\n  'loadingunload',\n  'bring',\n  'daughter',\n  'daughter',\n  'bring',\n  'daughter',\n  'given',\n  'notic',\n  'understand',\n  'repeal',\n  'untim',\n  'mistaken'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'serangoon',\n  'level',\n  'build',\n  'explan',\n  'offic',\n  'first',\n  'resid',\n  'locat',\n  'upstair',\n  'fetch',\n  'elderli',\n  'wheelchair',\n  'cannot',\n  'possibl',\n  'wheel',\n  'carpark',\n  'suppos',\n  'longer',\n  'elderli',\n  'definit',\n  'couldnt',\n  'probabl',\n  'right',\n  'obstruct',\n  'garbag',\n  'truck',\n  'right',\n  'minut',\n  'realli',\n  'consid',\n  'valid',\n  'reason',\n  'avoid',\n  'carpark',\n  'season',\n  'resid'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'level',\n  'build',\n  'explan',\n  'season',\n  'ticket',\n  'display'],\n ['provid',\n  'seraphina',\n  'chian',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'level',\n  'build',\n  'cheng',\n  'green',\n  'explan',\n  'resid',\n  'season',\n  'carpark',\n  'howev',\n  'involv',\n  'accid',\n  'requir',\n  'replac',\n  'report',\n  'polic',\n  'vehicl',\n  'number',\n  'anymor',\n  'detail',\n  'gladli',\n  'provid'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'level',\n  'build',\n  'cheng',\n  'green',\n  'explan',\n  'current',\n  'oversea',\n  'temporari',\n  'transfer',\n  'season',\n  'durat',\n  'start',\n  'howev',\n  'assist',\n  'check',\n  'misunderstand'],\n ['provid',\n  'cheng',\n  'chong',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'level',\n  'build',\n  'explan',\n  'actual',\n  'season',\n  'holder',\n  'carpark',\n  'oversea',\n  'forgot',\n  'renew',\n  'alreadi',\n  'renew',\n  'season'],\n ['provid',\n  'rabiulaw',\n  'kitang',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'punggol',\n  'field',\n  'level',\n  'build',\n  'nautilu',\n  'punggol',\n  'explan',\n  'would',\n  'lenienc',\n  'vehicl',\n  'permit',\n  'reason',\n  'beacaus',\n  'realli',\n  'toilet',\n  'urgent',\n  'stomach',\n  'reliv',\n  'drive',\n  'along',\n  'punggol',\n  'quickli',\n  'nearbi',\n  'coffeeshop',\n  'without',\n  'notic',\n  'carpark',\n  'singaporean',\n  'ensur',\n  'compli',\n  'carpark',\n  'futur',\n  'realli',\n  'lenienc',\n  'summon',\n  'reduc',\n  'helplessli',\n  'amatt'],\n ['provid',\n  'palaniappan',\n  'senthilkumar',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'jalan',\n  'songket',\n  'level',\n  'build',\n  'charlton',\n  'explan',\n  'involv',\n  'neighbor',\n  'space',\n  'unload',\n  'longer',\n  'usual',\n  'assur',\n  'mistak',\n  'happen',\n  'ensur',\n  'regul',\n  'futur',\n  'alway',\n  'regard'],\n ['provid',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'payoh',\n  'level',\n  'build',\n  'payoh',\n  'north',\n  'explan',\n  'first',\n  'admit',\n  'mistak',\n  'exceed',\n  'valid',\n  'coupon',\n  'place',\n  'vehicl',\n  'coupon',\n  'valid',\n  'exceed',\n  'reconsider',\n  'amount',\n  'accept',\n  'amount',\n  'exceed',\n  'mistak'],\n ['sirmadam',\n  'famili',\n  'bereav',\n  'respect',\n  'request',\n  'consid',\n  'rescind',\n  'summon',\n  'illeg',\n  'avoid',\n  'settl',\n  'stuff',\n  'around',\n  'minut',\n  'gantri',\n  'properli',\n  'design',\n  'first',\n  'sudden',\n  'death',\n  'famili',\n  'funer',\n  'arriv',\n  'pavilion',\n  'unload',\n  'would',\n  'realli',\n  'appreci',\n  'understand',\n  'matter',\n  'sincer'],\n ['provid',\n  'karen',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'keppel',\n  'level',\n  'build',\n  'reflect',\n  'keppel',\n  'explan',\n  'carpark',\n  'overflow',\n  'close',\n  'estim',\n  'vehicl',\n  'carpark',\n  'visit',\n  'carpark',\n  'medic',\n  'appoint',\n  'thomson',\n  'clinic',\n  'recent',\n  'injur',\n  'nearest',\n  'clinic',\n  'close',\n  'minut',\n  'imposs',\n  'carpark',\n  'vehicl',\n  'choic',\n  'season',\n  'think',\n  'medic',\n  'return',\n  'ticket',\n  'alreadi',\n  'recent',\n  'health',\n  'reason',\n  'understand',\n  'circumst',\n  'pertain',\n  'approv',\n  'result'],\n ['provid',\n  'melvin',\n  'identif',\n  'nopassport',\n  'blkhous',\n  'street',\n  'payoh',\n  'level',\n  'build',\n  'payoh',\n  'explan',\n  'resid',\n  'block',\n  'would',\n  'quickli',\n  'bring',\n  'elder',\n  'heavi',\n  'would',\n  'physic',\n  'challeng',\n  'carri',\n  'manag',\n  'carpark',\n  'without',\n  'compromis',\n  'safeti',\n  'reason',\n  'servic',\n  'avail',\n  'unload',\n  'point',\n  'apolog',\n  'inconveni',\n  'avoid',\n  'creat',\n  'obstruct',\n  'forward',\n  'appreci',\n  'could',\n  'attent']]"
                    }, 
                    "execution_count": 67, 
                    "metadata": {}
                }
            ], 
            "execution_count": 67
        }, 
        {
            "source": "#### Checking the frequency distribution and most common words again on the new list gives more sensible results and shows the major characters in a book.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "fdist=FreqDist(vocab_train)\nm=fdist.most_common(10)\nm", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[('level', 58),\n ('provid', 58),\n ('build', 57),\n ('explan', 57),\n ('blkhous', 57),\n ('nopassport', 57),\n ('street', 57),\n ('identif', 57),\n ('vehicl', 41),\n ('carpark', 38)]"
                    }, 
                    "execution_count": 8, 
                    "metadata": {}
                }
            ], 
            "execution_count": 8
        }, 
        {
            "source": "## Import Brunel library for visualization", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import brunel", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 9
        }, 
        {
            "source": "### Create a dataframe to visualize the common words as a tag cloud using Brunel package.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import pandas as pd\ndf = pd.DataFrame(columns=['word', 'freq'])\nfor i in m:\n    df.loc[len(df)] = i\n    \nprint (df)\n        ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "         word  freq\n0       level  58.0\n1      provid  58.0\n2       build  57.0\n3      explan  57.0\n4     blkhous  57.0\n5  nopassport  57.0\n6      street  57.0\n7     identif  57.0\n8      vehicl  41.0\n9     carpark  38.0\n"
                }
            ], 
            "execution_count": 10
        }, 
        {
            "source": "## Tag cloud of most common words", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "%%brunel cloud color(freq) size(freq) sort(freq)\nlabel(word) style('font-size:200px;font-family:Impact') legends(none) :: width = 600, height=600", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>", 
                        "text/html": "<!--\n  ~ Copyright (c) 2015 IBM Corporation and others.\n  ~\n  ~ Licensed under the Apache License, Version 2.0 (the \"License\");\n  ~ You may not use this file except in compliance with the License.\n  ~ You may obtain a copy of the License at\n  ~\n  ~     http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing, software\n  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  ~ See the License for the specific language governing permissions and\n  ~ limitations under the License.\n  -->\n\n\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/data/jupyter2/1be666d4-335c-4301-a9ac-efa7ddefc457/nbextensions/brunel_ext/brunel.2.3.css\">\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/data/jupyter2/1be666d4-335c-4301-a9ac-efa7ddefc457/nbextensions/brunel_ext/sumoselect.css\">\n\n<style>\n    #visidb89fd6fc-f112-11e7-ac28-002590fb6604.brunel .chart1 .element1 .element {\n\tfont-family: Impact;\n\tfont-size: 200px;\n}\n</style>\n\n<div id=\"controlsidb89fd8e6-f112-11e7-ac28-002590fb6604\" class=\"brunel\"/>\n<svg id=\"visidb89fd6fc-f112-11e7-ac28-002590fb6604\" width=\"600\" height=\"600\"></svg>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "application/javascript": "/*\n * Copyright (c) 2015 IBM Corporation and others.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * You may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nrequire.config({\n    waitSeconds: 60,\n    paths: {\n        'd3': '//cdnjs.cloudflare.com/ajax/libs/d3/4.2.1/d3.min',\n        'topojson': '//cdnjs.cloudflare.com/ajax/libs/topojson/1.6.20/topojson.min',\n        'brunel' : '/data/jupyter2/1be666d4-335c-4301-a9ac-efa7ddefc457/nbextensions/brunel_ext/brunel.2.3.min',\n        'brunelControls' : '/data/jupyter2/1be666d4-335c-4301-a9ac-efa7ddefc457/nbextensions/brunel_ext/brunel.controls.2.3.min'\n    },\n    shim: {\n       'brunel' : {\n            exports: 'BrunelD3',\n            deps: ['d3', 'topojson'],\n            init: function() {\n               return {\n                 BrunelD3 : BrunelD3,\n                 BrunelData : BrunelData\n              }\n            }\n        },\n       'brunelControls' : {\n            exports: 'BrunelEventHandlers',\n            init: function() {\n               return {\n                 BrunelEventHandlers: BrunelEventHandlers,\n                 BrunelJQueryControlFactory: BrunelJQueryControlFactory\n              }\n            }\n        }\n\n    }\n\n});\n\nrequire([\"d3\"], function(d3) {\n    require([\"brunel\", \"brunelControls\"], function(brunel, brunelControls) {\n        function  BrunelVis(visId) {\n  \"use strict\";                                                                       // strict mode\n  var datasets = [],                                      // array of datasets for the original data\n      pre = function(d, i) { return d },                         // default pre-process does nothing\n      post = function(d, i) { return d },                       // default post-process does nothing\n      transitionTime = 200,                                        // transition time for animations\n      charts = [],                                                       // the charts in the system\n      vis = d3.select('#' + visId).attr('class', 'brunel');                     // the SVG container\n\n  BrunelD3.addDefinitions(vis);                                   // ensure standard symbols present\n\n  // Define chart #1 in the visualization //////////////////////////////////////////////////////////\n\n  charts[0] = function(parentNode, filterRows) {\n    var geom = BrunelD3.geometry(parentNode || vis.node(), 0, 0, 1, 1, 0, 0, 0, 0),\n      elements = [];                                              // array of elements in this chart\n\n    // Define groups for the chart parts ///////////////////////////////////////////////////////////\n\n    var chart =  vis.append('g').attr('class', 'chart1')\n      .attr('transform','translate(' + geom.chart_left + ',' + geom.chart_top + ')');\n    var overlay = chart.append('g').attr('class', 'element').attr('class', 'overlay');\n    var zoom = d3.zoom().scaleExtent([1/3,3]);\n    var zoomNode = overlay.append('rect').attr('class', 'overlay')\n      .attr('x', geom.inner_left).attr('y', geom.inner_top)\n      .attr('width', geom.inner_rawWidth).attr('height', geom.inner_rawHeight)\n      .style('cursor', 'default')\n      .node();\n    zoomNode.__zoom = d3.zoomIdentity;\n    chart.append('rect').attr('class', 'background').attr('width', geom.chart_right-geom.chart_left).attr('height', geom.chart_bottom-geom.chart_top);\n    var interior = chart.append('g').attr('class', 'interior zoomNone')\n      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')')\n      .attr('clip-path', 'url(#clip_visidb89fd6fc-f112-11e7-ac28-002590fb6604_chart1_inner)');\n    interior.append('rect').attr('class', 'inner').attr('width', geom.inner_width).attr('height', geom.inner_height);\n    var gridGroup = interior.append('g').attr('class', 'grid');\n    vis.append('clipPath').attr('id', 'clip_visidb89fd6fc-f112-11e7-ac28-002590fb6604_chart1_inner').append('rect')\n      .attr('x', 0).attr('y', 0)\n      .attr('width', geom.inner_rawWidth+1).attr('height', geom.inner_rawHeight+1);\n    var scale_x = d3.scaleLinear(), scale_y = d3.scaleLinear();\n    var base_scales = [scale_x, scale_y];                           // untransformed original scales\n    zoom.on('zoom', function(t, time) {\n        t = t || d3.event.transform;\n        zoomNode.__zoom = t;\n        interior.attr('class', 'interior ' + BrunelD3.zoomLabel(t.k));;\n        interior.attr('transform', d3.zoomTransform(zoomNode));\n    });\n\n    // Define element #1 ///////////////////////////////////////////////////////////////////////////\n\n    elements[0] = function() {\n      var original, processed,                           // data sets passed in and then transformed\n        element, data,                                 // brunel element information and brunel data\n        selection, merged;                                      // d3 selection and merged selection\n      var elementGroup = interior.append('g').attr('class', 'element1')\n        .attr('transform','translate(' + geom.inner_width/2 + ',' + geom.inner_height/2 + ')'),\n        main = elementGroup.append('g').attr('class', 'main'),\n        labels = BrunelD3.undoTransform(elementGroup.append('g').attr('class', 'labels').attr('aria-hidden', 'true'), elementGroup);\n\n      function makeData() {\n        original = datasets[0];\n        if (filterRows) original = original.retainRows(filterRows);\n        processed = pre(original, 0)\n          .sort('freq');\n        processed = post(processed, 0);\n        var f0 = processed.field('freq'),\n          f1 = processed.field('word'),\n          f2 = processed.field('#row'),\n          f3 = processed.field('#selection');\n        var keyFunc = function(d) { return f2.value(d) };\n        data = {\n          freq:         function(d) { return f0.value(d.row) },\n          word:         function(d) { return f1.value(d.row) },\n          $row:         function(d) { return f2.value(d.row) },\n          $selection:   function(d) { return f3.value(d.row) },\n          freq_f:       function(d) { return f0.valueFormatted(d.row) },\n          word_f:       function(d) { return f1.valueFormatted(d.row) },\n          $row_f:       function(d) { return f2.valueFormatted(d.row) },\n          $selection_f: function(d) { return f3.valueFormatted(d.row) },\n          _split:       function(d) { return f0.value(d.row)+ '|' + f0.value(d.row) },\n          _key:         keyFunc,\n          _rows:        BrunelD3.makeRowsWithKeys(keyFunc, processed.rowCount())\n        };\n      }\n      // Aesthetic Functions\n      var scale_color = d3.scaleLinear().domain([38, 40.5, 43, 45.5, 48, 50.5, 53, 55.5, 58])\n        .interpolate(d3.interpolateHcl)\n        .range([ '#045a8d', '#2b8cbe', '#74a9cf', '#bdc9e1', '#f8efe8', '#fef0d9', \n          '#fdcc8a', '#fc8d59', '#e34a33']);\n      var color = function(d) { return scale_color(data.freq(d)) };\n      var scale_size = d3.scaleSqrt().domain([0, 58.000006])\n        .range([ 0.001, 1]);\n      var size = function(d) { return scale_size(data.freq(d)) };\n\n      // Build element from data ///////////////////////////////////////////////////////////////////\n\n      function build(transitionMillis) {\n        element = elements[0];\n        // Build the cloud layout\n        var cloud = BrunelD3.cloudLayout(processed, [geom.inner_width, geom.inner_height], zoomNode);\n        function keyFunction(d) { return d.key };\n        main.attr('class', 'diagram cloud');\n\n        // Define selection entry operations\n        function initialState(selection) {\n          selection\n            .attr('class', 'element text filled')\n            .style('text-anchor', 'middle').classed('label', true)\n            .text(function(d) { return data.word_f(d) })\n            .style('font-size', function(d) { return (100*size(d)) + '%' })\n            .style('pointer-events', 'none')\n        }\n\n        // Define selection update operations on merged data\n        function updateState(selection) {\n          selection\n            .each(cloud.prepare).call(cloud.build)\n            .filter(BrunelD3.hasData)                     // following only performed for data items\n            .style('fill', color);\n        }\n        // Create selections, set the initial state and transition updates\n        selection = main.selectAll('.element').data(data._rows, function(d) { return d.key });\n        var added = selection.enter().append('text');\n        merged = selection.merge(added);\n        initialState(added);\n        selection.filter(BrunelD3.hasData)\n          .classed('selected', BrunelD3.isSelected(data))\n          .filter(BrunelD3.isSelected(data)).raise();\n        updateState(BrunelD3.transition(merged, transitionMillis));\n\n        BrunelD3.transition(selection.exit(), transitionMillis/3)\n          .style('opacity', 0.5).each( function() {\n            this.remove(); BrunelD3.removeLabels(this); \n        });\n      }\n\n      return {\n        data:           function() { return processed },\n        original:       function() { return original },\n        internal:       function() { return data },\n        selection:      function() { return merged },\n        makeData:       makeData,\n        build:          build,\n        chart:          function() { return charts[0] },\n        group:          function() { return elementGroup },\n        fields: {\n          key:          ['#row'],\n          color:        ['freq'],\n          size:         ['freq']\n        }\n      };\n    }();\n\n    function build(time, noData) {\n      var first = elements[0].data() == null;\n      if (first) time = 0;                                           // no transition for first call\n      if ((first || time > -1) && !noData) {\n        elements[0].makeData();\n      }\n      elements[0].build(time);\n    }\n\n    // Expose the following components of the chart\n    return {\n      elements : elements,\n      interior : interior,\n      zoom: function(params, time) {\n          if (params) zoom.on('zoom').call(zoomNode, params, time);\n          return d3.zoomTransform(zoomNode);\n      },\n      build : build\n    };\n    }();\n\n  function setData(rowData, i) { datasets[i||0] = BrunelD3.makeData(rowData) }\n  function updateAll(time) { charts.forEach(function(x) {x.build(time || 0)}) }\n  function buildAll() {\n    for (var i=0;i<arguments.length;i++) setData(arguments[i], i);\n    updateAll(transitionTime);\n  }\n\n  return {\n    dataPreProcess:     function(f) { if (f) pre = f; return pre },\n    dataPostProcess:    function(f) { if (f) post = f; return post },\n    data:               function(d,i) { if (d) setData(d,i); return datasets[i||0] },\n    visId:              visId,\n    build:              buildAll,\n    rebuild:            updateAll,\n    charts:             charts\n  }\n}\n\n// Data Tables /////////////////////////////////////////////////////////////////////////////////////\n\nvar table1 = {\n   summarized: false,\n   names: ['freq', 'word'], \n   options: ['numeric', 'string'], \n   rows: [[58, 'level'], [58, 'provid'], [57, 'build'], [57, 'explan'], [57, 'blkhous'],\n  [57, 'nopassport'], [57, 'street'], [57, 'identif'], [41, 'vehicl'], [38, 'carpark']]\n};\n\n// Call Code to Build the system ///////////////////////////////////////////////////////////////////\n\nvar v  = new BrunelVis('visidb89fd6fc-f112-11e7-ac28-002590fb6604');\nv.build(table1);\n\n    });\n});", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "execution_count": 11, 
                    "metadata": {}
                }
            ], 
            "execution_count": 11
        }, 
        {
            "source": "## Keras\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Embedding\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\n ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n"
                }
            ], 
            "execution_count": 12
        }, 
        {
            "source": "train_docs=[]\ntest_docs=[]\nfor d in data_train:\n    train_docs.append(\" \".join(d)) \nfor d in data_test:\n    test_docs.append(\" \".join(d))\n#print (len(docs))\n#train_docs=docs\n#train_docs\nprint (len(train_docs))\nprint (len(test_docs))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "63\n19\n"
                }
            ], 
            "execution_count": 13
        }, 
        {
            "source": "#create the tokenizer\ntokenizer = Tokenizer()\n# fit the tokenizer on the documents\ntokenizer.fit_on_texts(train_docs)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 14
        }, 
        {
            "source": "# sequence encode\nencoded_docs_train = tokenizer.texts_to_matrix(train_docs,mode='tfidf')\nencoded_docs_test = tokenizer.texts_to_matrix(test_docs,mode='tfidf')\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 15
        }, 
        {
            "source": "# pad sequences\n#max_length = max([len(s.split()) for s in train_docs])\nprint (max_length)\nmax_length=1000\nXtrain = pad_sequences(encoded_docs_train, maxlen=max_length, padding='post')\nXtest = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')\n#Xtrain\n#print(Xtrain.shape)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "1000\n"
                }
            ], 
            "execution_count": 55
        }, 
        {
            "source": "from numpy import array\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n#print (labels_train)\nvalues_train = array(labels_train)\n#print(values_train)\nvalues_test = array(labels_test)\n#print(values_test)\n# integer encode\nlabel_encoder = LabelEncoder()\ninteger_encoded_train = label_encoder.fit_transform(values_train)\ninteger_encoded_test = label_encoder.fit_transform(values_test)\n#print(integer_encoded)\nonehot_encoder = OneHotEncoder(sparse=False)\n#integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n#onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n#print(onehot_encoded)\nytrain=integer_encoded_train\nytest=integer_encoded_test\nprint (ytrain)\nprint (ytest)\n# define training labels\n#print (array([0 for _ in range(48)] + [1 for _ in range(5)]))\n#ytrain = array([1 for _ in range(5)] + [0 for _ in range(5)])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n[0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n"
                }
            ], 
            "execution_count": 56
        }, 
        {
            "source": "# define vocabulary size (largest integer value)\nvocab_size = len(tokenizer.word_index) + 1\nvocab_size", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "729"
                    }, 
                    "execution_count": 57, 
                    "metadata": {}
                }
            ], 
            "execution_count": 57
        }, 
        {
            "source": "model = Sequential()\nmodel.add(Embedding(vocab_size, 100, input_length=max_length))\nmodel.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nprint(model.summary())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_7 (Embedding)      (None, 1000, 100)         72900     \n_________________________________________________________________\nconv1d_7 (Conv1D)            (None, 993, 32)           25632     \n_________________________________________________________________\nmax_pooling1d_7 (MaxPooling1 (None, 496, 32)           0         \n_________________________________________________________________\nflatten_7 (Flatten)          (None, 15872)             0         \n_________________________________________________________________\ndense_13 (Dense)             (None, 50)                793650    \n_________________________________________________________________\ndense_14 (Dense)             (None, 1)                 51        \n=================================================================\nTotal params: 892,233\nTrainable params: 892,233\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
                }
            ], 
            "execution_count": 58
        }, 
        {
            "source": "# compile network\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit network\nmodel.fit(Xtrain, ytrain, epochs=50, verbose=2)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Epoch 1/50\n0s - loss: 7.3976e-04 - acc: 1.0000\nEpoch 2/50\n0s - loss: 0.0022 - acc: 1.0000\nEpoch 3/50\n0s - loss: 0.0081 - acc: 1.0000\nEpoch 4/50\n0s - loss: 1.5804e-04 - acc: 1.0000\nEpoch 5/50\n0s - loss: 0.0020 - acc: 1.0000\nEpoch 6/50\n0s - loss: 0.0046 - acc: 1.0000\nEpoch 7/50\n0s - loss: 0.0031 - acc: 1.0000\nEpoch 8/50\n0s - loss: 0.0023 - acc: 1.0000\nEpoch 9/50\n0s - loss: 0.0016 - acc: 1.0000\nEpoch 10/50\n0s - loss: 0.0011 - acc: 1.0000\nEpoch 11/50\n0s - loss: 8.1786e-04 - acc: 1.0000\nEpoch 12/50\n0s - loss: 5.0778e-04 - acc: 1.0000\nEpoch 13/50\n0s - loss: 3.6284e-04 - acc: 1.0000\nEpoch 14/50\n0s - loss: 2.5405e-04 - acc: 1.0000\nEpoch 15/50\n0s - loss: 1.7581e-04 - acc: 1.0000\nEpoch 16/50\n0s - loss: 1.6225e-04 - acc: 1.0000\nEpoch 17/50\n0s - loss: 1.4529e-04 - acc: 1.0000\nEpoch 18/50\n0s - loss: 1.3421e-04 - acc: 1.0000\nEpoch 19/50\n0s - loss: 1.2677e-04 - acc: 1.0000\nEpoch 20/50\n0s - loss: 1.1494e-04 - acc: 1.0000\nEpoch 21/50\n0s - loss: 8.5614e-05 - acc: 1.0000\nEpoch 22/50\n0s - loss: 8.0529e-05 - acc: 1.0000\nEpoch 23/50\n0s - loss: 6.6119e-05 - acc: 1.0000\nEpoch 24/50\n0s - loss: 6.7518e-05 - acc: 1.0000\nEpoch 25/50\n0s - loss: 6.4676e-05 - acc: 1.0000\nEpoch 26/50\n0s - loss: 5.7827e-05 - acc: 1.0000\nEpoch 27/50\n0s - loss: 5.0200e-05 - acc: 1.0000\nEpoch 28/50\n0s - loss: 4.4818e-05 - acc: 1.0000\nEpoch 29/50\n0s - loss: 4.1554e-05 - acc: 1.0000\nEpoch 30/50\n0s - loss: 4.0094e-05 - acc: 1.0000\nEpoch 31/50\n0s - loss: 3.9099e-05 - acc: 1.0000\nEpoch 32/50\n0s - loss: 3.6472e-05 - acc: 1.0000\nEpoch 33/50\n0s - loss: 3.5208e-05 - acc: 1.0000\nEpoch 34/50\n0s - loss: 3.2476e-05 - acc: 1.0000\nEpoch 35/50\n0s - loss: 3.1709e-05 - acc: 1.0000\nEpoch 36/50\n0s - loss: 2.8954e-05 - acc: 1.0000\nEpoch 37/50\n0s - loss: 2.7767e-05 - acc: 1.0000\nEpoch 38/50\n0s - loss: 2.6344e-05 - acc: 1.0000\nEpoch 39/50\n0s - loss: 2.6108e-05 - acc: 1.0000\nEpoch 40/50\n0s - loss: 2.4907e-05 - acc: 1.0000\nEpoch 41/50\n0s - loss: 2.3580e-05 - acc: 1.0000\nEpoch 42/50\n0s - loss: 2.2793e-05 - acc: 1.0000\nEpoch 43/50\n0s - loss: 2.2531e-05 - acc: 1.0000\nEpoch 44/50\n0s - loss: 2.1475e-05 - acc: 1.0000\nEpoch 45/50\n0s - loss: 2.0769e-05 - acc: 1.0000\nEpoch 46/50\n0s - loss: 2.0128e-05 - acc: 1.0000\nEpoch 47/50\n0s - loss: 1.9294e-05 - acc: 1.0000\nEpoch 48/50\n0s - loss: 1.8962e-05 - acc: 1.0000\nEpoch 49/50\n0s - loss: 1.8401e-05 - acc: 1.0000\nEpoch 50/50\n0s - loss: 1.7878e-05 - acc: 1.0000\n"
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7f932dbd0748>"
                    }, 
                    "execution_count": 65, 
                    "metadata": {}
                }
            ], 
            "execution_count": 65
        }, 
        {
            "source": "# evaluate\nloss, acc = model.evaluate(Xtest, ytest, verbose=0)\nprint('Test Accuracy: %f' % (acc*100))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Test Accuracy: 68.421054\n"
                }
            ], 
            "execution_count": 66
        }, 
        {
            "source": "import numpy as np\ntest_docs=[]\ntest=\"I acknowledged that the Driver's information provided are true. Name: VIOLET LEE Identification Type: Passport No. NRIC No./Passport No.: 550106771 Postal Code: 521109 Blk/House No.: 109 Street Name: TAMPINES ST 11 Level and Unit No.: #10-281 Building Name: Explanation: Dear Sir/Madam, I did not receive any summon notice on my car this day and was surprised to see that I have a fine of $100 with the above summon number when I logged in to check my vehicle for another summon. I don't know what offense I have made to be summoned. But on that day, I was dropping off my friend who was returning home after brain surgery and had to park at the unloading bay as it was the closest to the the entrance of her block. It was inconvenient for her to walk all the way from the multi storey car park after the surgery. And as her maid wasn't home yet, I had to help her up and made sure she was ok and waited for the maid to return before leaving. So if this summon is for parking at the unloading bay, I wish you can kindly consider my situation at that time and waive the summon charge. Thank you for your assistance and sorry for any inconvenience caused. Yours sincerely, Violet.\"\ntest_data=clean_doc(test)\ntest_docs=test_data\n#test_docs\nencoded_test_docs = tokenizer.texts_to_matrix(test_docs, mode='tfidf')\n#max_length = max([len(s.split()) for s in test_docs])\n#print (max_length)\n#Xtest = pad_sequences(encoded_test_docs, maxlen=max_length, padding='post')\n#Xtest\n#ytest = array([1])\n#loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n#print('Test Accuracy: %f' % (acc*100))\nyhat = model.predict(encoded_test_docs, verbose=0)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "error", 
                    "evalue": "Error when checking : expected embedding_13_input to have shape (None, 1000) but got array with shape (55, 892)", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-326-bc636b72ebce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#loss, acc = model.evaluate(Xtest, ytest, verbose=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#print('Test Accuracy: %f' % (acc*100))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_test_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[1;32m/usr/local/src/conda3_runtime.v25/4.1.1/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m    914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/conda3_runtime.v25/4.1.1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1574\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[0;32m   1575\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1576\u001b[1;33m                                     check_batch_axis=False)\n\u001b[0m\u001b[0;32m   1577\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1578\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/conda3_runtime.v25/4.1.1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                             \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m                             str(array.shape))\n\u001b[0m\u001b[0;32m    140\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;31mValueError\u001b[0m: Error when checking : expected embedding_13_input to have shape (None, 1000) but got array with shape (55, 892)"
                    ], 
                    "ename": "ValueError"
                }
            ], 
            "execution_count": 326
        }, 
        {
            "source": "def predict_sentiment(review, vocab, tokenizer, model):\n\t# clean\n\ttokens=clean_doc(review)\n    \n    #print (tokens)\n\t# filter by vocab\n\ttokens = [w for w in tokens if w in vocab]\n\t# convert to line\n\tline = ' '.join(tokens)\n\t# encode\n\tencoded = tokenizer.texts_to_matrix([line], mode='tfidf')\n\tmax_length = max([len(s.split()) for s in line])\n\tprint (max_length)\n\tXtest = pad_sequences(encoded, maxlen=max_length, padding='post')\n\t# prediction\n\tyhat = model.predict(encoded, verbose=0)\n\treturn round(yhat[0,0])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 220
        }, 
        {
            "source": "import numpy as np\nTEXT=np.array([\"I acknowledged that the Driver's information provided are true. Name: VIOLET LEE Identification Type: Passport No. NRIC No./Passport No.: 550106771 Postal Code: 521109 Blk/House No.: 109 Street Name: TAMPINES ST 11 Level and Unit No.: #10-281 Building Name: Explanation: Dear Sir/Madam, I did not receive any summon notice on my car this day and was surprised to see that I have a fine of $100 with the above summon number when I logged in to check my vehicle for another summon. I don't know what offense I have made to be summoned. But on that day, I was dropping off my friend who was returning home after brain surgery and had to park at the unloading bay as it was the closest to the the entrance of her block. It was inconvenient for her to walk all the way from the multi storey car park after the surgery. And as her maid wasn't home yet, I had to help her up and made sure she was ok and waited for the maid to return before leaving. So if this summon is for parking at the unloading bay, I wish you can kindly consider my situation at that time and waive the summon charge. Thank you for your assistance and sorry for any inconvenience caused. Yours sincerely, Violet.\"])\n#print(predict_sentiment(test, vocab, tokenizer, model))\n\n#tokens=clean_doc(test)\nMAX_SEQUENCE_LENGTH=1000\nSEQUENCES = tokenizer.texts_to_sequences(TEXT)\n#max_length = max([len(s.split()) for s in TEXT])\nDATA = pad_sequences(SEQUENCES, maxlen=MAX_SEQUENCE_LENGTH)\nprint (DATA)\nPREDICTION = model.predict(DATA,verbose=0)\n#print('result: ' + np.array(LABELS)[PREDICTION.argmax(axis=1)][0])\n\nround(PREDICTION[0,0])\n#PREDICTION", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   7   9   4 443  14  14\n   26  43  14  76  27 125  14  45  14  13]]\n"
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "0.0"
                    }, 
                    "execution_count": 36, 
                    "metadata": {}
                }
            ], 
            "execution_count": 36
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.2", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}