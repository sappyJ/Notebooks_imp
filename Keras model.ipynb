{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# This notebok demonstrates how to use python NLTK package for text cleaning and text preparation. It also shows how to perform cosine similarity to find similar documents.\n\n## 2017 Dec Shilpa Jain", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Install Python NLTK package\n\nNLTK is a natural language toolkit for building programs in Python that work with natural language text.\nWe will use NLTK for this course.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!pip install nltk --upgrade", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting nltk\n  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.4MB 662kB/s eta 0:00:01\n\u001b[?25hRequirement already up-to-date: six in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s9ac-efa7ddefc45728-3c618564d05f/.local/lib/python3.5/site-packages (from nltk)\nBuilding wheels for collected packages: nltk\n  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /gpfs/fs01/user/s9ac-efa7ddefc45728-3c618564d05f/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\nSuccessfully built nltk\nInstalling collected packages: nltk\n  Found existing installation: nltk 3.2.5\n    Uninstalling nltk-3.2.5:\n      Successfully uninstalled nltk-3.2.5\nSuccessfully installed nltk-3.3\n"
                }
            ], 
            "execution_count": 1
        }, 
        {
            "source": "## Import NLTK and download NLTK book collection", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import nltk\n\nnltk.download('punkt')\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[nltk_data] Downloading package punkt to /gpfs/fs01/user/s9ac-\n[nltk_data]     efa7ddefc45728-3c618564d05f/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "True"
                    }, 
                    "execution_count": 2, 
                    "metadata": {}
                }
            ], 
            "execution_count": 2
        }, 
        {
            "source": "## Cell below will load all the items in the book module that you have just downloaded. When this finishes, we will see the output.\nWe can see from the output that there are 9 pieces of text and 9 sentences loaded. For example, if we\ntype text1, we will see the title of the first piece of text. If we type sent3, we will see the body of the\nthird sentence.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\nimport sys\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share your notebook.\nclient_8760db995d144d1cab8bb99f8e30e4d7 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='Rx3KcqqyVM0CvH2mcoC-oNhWl4AIgJNnLG4yj6qWbLXG',\n    ibm_service_instance_id=\"iam-ServiceId-80092a7b-a6ab-4262-b7f0-11b4bbb372ac\",\n    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client_8760db995d144d1cab8bb99f8e30e4d7.get_object(Bucket='hdbc3bf4c3bf3454028bc34d6d765ab9a09',Key='HDB.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf_data_3 = pd.read_csv(body,encoding='utf-8')\ndf_data_train=df_data_3[:63]\ndf_data_test=df_data_3[64:]\ndf_data_train\ndf_data_test\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "    Case                                               Data     Label\n64    65  I acknowledged that the Driver's information p...  Rejected\n65    66  I acknowledged that the Driver's information p...  Rejected\n66    67  I acknowledged that the Driver's information p...  Rejected\n67    68  I acknowledged that the Driver's information p...  Rejected\n68    69  I acknowledged that the Driver's information p...  Rejected\n69    70  I acknowledged that the Driver's information p...  Rejected\n70    71  I acknowledged that the Driver's information p...  Rejected\n71    72  I acknowledged that the Driver's information p...  Rejected\n72    73  I acknowledged that the Driver's information p...  Rejected\n73    74  I acknowledged that the Driver's information p...    Waived\n74    75  I acknowledged that the Driver's information p...    Waived\n75    76  I acknowledged that the Driver's information p...    Waived\n76    77  I acknowledged that the Driver's information p...    Waived\n77    78  I acknowledged that the Driver's information p...    Waived\n78    79  I acknowledged that the Driver's information p...    Waived\n79    80  I acknowledged that the Driver's information p...    Waived\n80    81  I acknowledged that the Driver's information p...    Waived\n81    82  I acknowledged that the Driver's information p...    Waived\n82    83  I,Ting Kok Choon I/c no:S1171455B is the drive...    Waived", 
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Case</th>\n      <th>Data</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>64</th>\n      <td>65</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Rejected</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>66</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Rejected</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>67</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Rejected</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>68</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Rejected</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>69</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Rejected</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>70</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Rejected</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>71</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Rejected</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>72</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Rejected</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>73</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Rejected</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>74</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Waived</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>75</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Waived</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>76</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Waived</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>77</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Waived</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>78</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Waived</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>79</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Waived</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>80</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Waived</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>81</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Waived</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>82</td>\n      <td>I acknowledged that the Driver's information p...</td>\n      <td>Waived</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>83</td>\n      <td>I,Ting Kok Choon I/c no:S1171455B is the drive...</td>\n      <td>Waived</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    }, 
                    "execution_count": 3, 
                    "metadata": {}
                }
            ], 
            "execution_count": 3
        }, 
        {
            "source": "import nltk\ndocs_train=[]\ndocs_test=[]\nlabels_train=[]\nlabels_test=[]\nfor idx, row in df_data_train.iterrows():\n    #print (row['Label'])\n    \n    tokens = nltk.word_tokenize(row['Data'])\n   \n    docs_train.append(row['Data'])\n    labels_train.append(row['Label'])\n    text2_train = nltk.Text(tokens)\n    #docs.append(tokens)\nprint (len(docs_train))\n\nfor idx, row in df_data_test.iterrows():\n    #print (row['Label'])\n    \n    tokens = nltk.word_tokenize(row['Data'])\n    docs_test.append(row['Data'])\n    labels_test.append(row['Label'])\n    text2_test = nltk.Text(tokens)\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "63\n"
                }
            ], 
            "execution_count": 4
        }, 
        {
            "source": "##### In NLTK, there is a method called concordance that allows us to search for a word inside a piece of text.\n##### Count method returns the number of times a word occurs in a piece of text.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## UsingWord Counts to Obtain an Overview of a Collection\nAssume that you have a large document collection. For example, it could be all the email enquiries\nfrom the customers of a company in a particular month. It could be all the tweets published by a particular\nuser. It could also be all the fictions written by a particular author. Without going through all the documents\ninside the collection, how can you quickly get an idea about the major topics or themes covered by these\ndocuments?\n\nIn NLTK, there is a built-in function called FreqDist() that makes our task very easy.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from nltk import *\nprint (text2_train)\nfdist=FreqDist(text2_train)\ntype(fdist)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "<Text: I acknowledged that the Driver 's information provided...>\n"
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "nltk.probability.FreqDist"
                    }, 
                    "execution_count": 5, 
                    "metadata": {}
                }
            ], 
            "execution_count": 5
        }, 
        {
            "source": "##### There is a method called most_common() that can be conveniently used to show the most frequent words in a frequency distribution.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "fdist.most_common(10)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[('.', 12),\n ('the', 9),\n (':', 9),\n ('to', 5),\n ('I', 5),\n ('No', 4),\n ('for', 3),\n ('and', 3),\n ('Name', 3),\n ('TOA', 2)]"
                    }, 
                    "execution_count": 6, 
                    "metadata": {}
                }
            ], 
            "execution_count": 6
        }, 
        {
            "source": "#### Looking at the most frequent words, you realize that they are not so meaningful. Many words are so commonly used everywhere that they do not reveal anything about the particular document or document collection we are looking at. There are a number of ways to address this problem.\n\n#### We will create a new list text2_long_words and add only words with atleast 5 characters.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!pip install gensim", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Requirement already satisfied: gensim in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s9ac-efa7ddefc45728-3c618564d05f/.local/lib/python3.5/site-packages\nRequirement already satisfied: smart-open>=1.2.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s9ac-efa7ddefc45728-3c618564d05f/.local/lib/python3.5/site-packages (from gensim)\nRequirement already satisfied: scipy>=0.18.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s9ac-efa7ddefc45728-3c618564d05f/.local/lib/python3.5/site-packages (from gensim)\nRequirement already satisfied: six>=1.5.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s9ac-efa7ddefc45728-3c618564d05f/.local/lib/python3.5/site-packages (from gensim)\nRequirement already satisfied: numpy>=1.11.3 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s9ac-efa7ddefc45728-3c618564d05f/.local/lib/python3.5/site-packages (from gensim)\nRequirement already satisfied: boto>=2.32 in /usr/local/src/conda3_runtime.v37/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\nRequirement already satisfied: requests in /usr/local/src/conda3_runtime.v37/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\nRequirement already satisfied: bz2file in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s9ac-efa7ddefc45728-3c618564d05f/.local/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\nRequirement already satisfied: boto3 in /usr/local/src/conda3_runtime.v37/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/src/conda3_runtime.v37/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\nRequirement already satisfied: idna<2.7,>=2.5 in /usr/local/src/conda3_runtime.v37/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\nRequirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/src/conda3_runtime.v37/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/src/conda3_runtime.v37/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\nRequirement already satisfied: botocore<1.8.0,>=1.7.0 in /usr/local/src/conda3_runtime.v37/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/src/conda3_runtime.v37/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim)\nRequirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/src/conda3_runtime.v37/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/src/conda3_runtime.v37/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from botocore<1.8.0,>=1.7.0->boto3->smart-open>=1.2.1->gensim)\nRequirement already satisfied: docutils>=0.10 in /usr/local/src/conda3_runtime.v37/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from botocore<1.8.0,>=1.7.0->boto3->smart-open>=1.2.1->gensim)\n"
                }
            ], 
            "execution_count": 7
        }, 
        {
            "source": "from nltk.corpus import stopwords\nfrom nltk.stem.porter import *\nstemmer=PorterStemmer()\nimport gensim\nfrom gensim import corpora\nimport string\nfrom string import punctuation\nstoplist=['acknowledged','acknowledgement','drivers','carpark','information','parking','driving','inform','Thanks','park','thank','postal','unit','appeal','driver','offence',\"Driver's\",'Driver','name','nric']\n\n# Input to dictionary is a list of list\n\n# turn a doc into clean tokens\ndef clean_doc(doc):\n\t# split into tokens by white space\n\ttokens = doc.split()\n\t# remove punctuation from each token\n    \n\ttable = str.maketrans('', '', string.punctuation)\n\ttokens = [w.translate(table) for w in tokens]\n\t# remove remaining tokens that are not alphabetic\n\ttokens = [word for word in tokens if word.isalpha()]\n\t# filter out stop words\n\ttags=nltk.pos_tag(tokens)\n\tprint(word[1])  \n\ttokens=[word[0] for word in tags if word[1].startswith('V') or word[1].startswith('N')]\n\tstop_words = set(stopwords.words('english'))\n\ttokens = [w for w in tokens if not w in stop_words]\n\ttokens = [w.lower() for w in tokens]\n\ttokens = [w for w in tokens if not w in stoplist]\n\ttokens=[stemmer.stem(w) for w in tokens]\n\ttokens = [w for w in tokens if not w in stoplist]\n\t# filter out short tokens\n\ttokens = [word for word in tokens if len(word) > 4]\n\treturn tokens\n    \n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 25
        }, 
        {
            "source": "print(df_data_train[['Data'],:2])\n#clean_doc(df_data_train['Data'])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "error", 
                    "evalue": "unhashable type: 'list'", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-9-9f462ea75517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_data_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#clean_doc(df_data_train['Data'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;34m\"\"\"Return the cached item, item represents a label indexer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
                    ], 
                    "ename": "TypeError"
                }
            ], 
            "execution_count": 9
        }, 
        {
            "source": "#nltk.download('stopwords')\nfrom collections import Counter\nvocab_train = Counter()\nvocab_test=Counter()\ndata_train=[]\ndata_test=[]\nfor i in docs_train:\n    print (type(i))\n    tokens=clean_doc(i)\n    print (tokens)\n    data_train.append(tokens)\n    vocab_train.update(tokens)\nprint(len(data_train))    \nfor i in docs_test:\n    tokens=clean_doc(i)\n    data_test.append(tokens)\n    vocab_test.update(tokens)\nprint(len(vocab_test))\n# print the top words in the vocab\nprint(vocab_train.most_common(50))\nm=vocab_train.most_common(50)\ndata_train\n#print (dictionary)\n#print (vocab)\n#print (content)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "<class 'str'>\n"
                }, 
                {
                    "output_type": "error", 
                    "evalue": "name 'word' is not defined", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-24-0879dd1e1a7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdata_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m<ipython-input-23-c506b3590db9>\u001b[0m in \u001b[0;36mclean_doc\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# filter out stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'V'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'N'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mNameError\u001b[0m: name 'word' is not defined"
                    ], 
                    "ename": "NameError"
                }
            ], 
            "execution_count": 24
        }, 
        {
            "source": "#### Checking the frequency distribution and most common words again on the new list gives more sensible results and shows the major characters in a book.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "fdist=FreqDist(vocab_train)\nm=fdist.most_common(50)\nm", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Import Brunel library for visualization", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import brunel", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### Create a dataframe to visualize the common words as a tag cloud using Brunel package.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import pandas as pd\ndf = pd.DataFrame(columns=['word', 'freq'])\nfor i in m:\n    df.loc[len(df)] = i\n    \nprint (df)\n        ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "error", 
                    "evalue": "name 'm' is not defined", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-11-64c0bbb63940>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'freq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
                    ], 
                    "ename": "NameError"
                }
            ], 
            "execution_count": 11
        }, 
        {
            "source": "## Tag cloud of most common words", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "%%brunel cloud color(freq) size(freq) sort(freq)\nlabel(word) style('font-size:200px;font-family:Impact') legends(none) :: width = 600, height=600", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Keras\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Embedding\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\n ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n"
                }
            ], 
            "execution_count": 12
        }, 
        {
            "source": "train_docs=[]\ntest_docs=[]\nfor d in data_train:\n    train_docs.append(\" \".join(d)) \nfor d in data_test:\n    test_docs.append(\" \".join(d))\n#print (len(docs))\n#train_docs=docs\n#train_docs\nprint (len(train_docs))\nprint (len(test_docs))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "0\n0\n"
                }
            ], 
            "execution_count": 13
        }, 
        {
            "source": "#create the tokenizer\ntokenizer = Tokenizer()\n# fit the tokenizer on the documents\ntokenizer.fit_on_texts(train_docs)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 17
        }, 
        {
            "source": "# sequence encode\nencoded_docs_train = tokenizer.texts_to_matrix(train_docs,mode='tfidf')\nencoded_docs_test = tokenizer.texts_to_matrix(test_docs,mode='tfidf')\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "error", 
                    "evalue": "Specify a dimension (num_words argument), or fit on some text data first.", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-18-1d75622b43d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# sequence encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mencoded_docs_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_docs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mencoded_docs_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_docs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_matrix\u001b[0;34m(self, texts, mode)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \"\"\"\n\u001b[1;32m    217\u001b[0m         \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msequences_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36msequences_to_matrix\u001b[0;34m(self, sequences, mode)\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0mnum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 raise ValueError('Specify a dimension (num_words argument), '\n\u001b[0m\u001b[1;32m    240\u001b[0m                                  'or fit on some text data first.')\n\u001b[1;32m    241\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mValueError\u001b[0m: Specify a dimension (num_words argument), or fit on some text data first."
                    ], 
                    "ename": "ValueError"
                }
            ], 
            "execution_count": 18
        }, 
        {
            "source": "# pad sequences\n#max_length = max([len(s.split()) for s in train_docs])\n#print (max_length)\nmax_length=1000\nXtrain = pad_sequences(encoded_docs_train, maxlen=max_length, padding='post')\nXtest = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')\n#Xtrain\n#print(Xtrain.shape)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "error", 
                    "evalue": "name 'encoded_docs_train' is not defined", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-16-757ebe2fc7cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print (max_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mXtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_docs_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mXtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_docs_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Xtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mNameError\u001b[0m: name 'encoded_docs_train' is not defined"
                    ], 
                    "ename": "NameError"
                }
            ], 
            "execution_count": 16
        }, 
        {
            "source": "from numpy import array\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n#print (labels_train)\nvalues_train = array(labels_train)\n#print(values_train)\nvalues_test = array(labels_test)\n#print(values_test)\n# integer encode\nlabel_encoder = LabelEncoder()\ninteger_encoded_train = label_encoder.fit_transform(values_train)\ninteger_encoded_test = label_encoder.fit_transform(values_test)\n#print(integer_encoded)\nonehot_encoder = OneHotEncoder(sparse=False)\n#integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n#onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n#print(onehot_encoded)\nytrain=integer_encoded_train\nytest=integer_encoded_test\nprint (ytrain)\nprint (ytest)\n# define training labels\n#print (array([0 for _ in range(48)] + [1 for _ in range(5)]))\n#ytrain = array([1 for _ in range(5)] + [0 for _ in range(5)])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n[0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n"
                }
            ], 
            "execution_count": 19
        }, 
        {
            "source": "# define vocabulary size (largest integer value)\nvocab_size = len(tokenizer.word_index) + 1\nvocab_size", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "1"
                    }, 
                    "execution_count": 20, 
                    "metadata": {}
                }
            ], 
            "execution_count": 20
        }, 
        {
            "source": "\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 200, input_length=max_length))\nmodel.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nprint(model.summary())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 1000, 200)         200       \n_________________________________________________________________\nconv1d_1 (Conv1D)            (None, 993, 32)           51232     \n_________________________________________________________________\nmax_pooling1d_1 (MaxPooling1 (None, 496, 32)           0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 15872)             0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 50)                793650    \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 51        \n=================================================================\nTotal params: 845,133\nTrainable params: 845,133\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
                }
            ], 
            "execution_count": 21
        }, 
        {
            "source": "# compile network\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit network\nmodel.fit(Xtrain, ytrain, epochs=50, verbose=2)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "error", 
                    "evalue": "name 'Xtrain' is not defined", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-22-9b2e260f6bb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# fit network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m: name 'Xtrain' is not defined"
                    ], 
                    "ename": "NameError"
                }
            ], 
            "execution_count": 22
        }, 
        {
            "source": "# evaluate\nloss, acc = model.evaluate(Xtest, ytest, verbose=0)\nprint('Test Accuracy: %f' % (acc*100))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import numpy as np\nTEXT=\"I acknowledged that the Driver's information provided are true. Name: GOH ZHI XIN, MOSES JAMES Identification Type: NRIC No. NRIC No./Passport No.: S8633130F Postal Code: 314079 Blk/House No.: 79D Street Name: TOA PAYOH CTRL Level and Unit No.: #34-45 Building Name: CENTRAL HORIZON Explanation: Dear Sir, I am writing to appeal to the parking offence issued on 9 August 2017. My wife and I are moving into this neighbourhood in Toa Payoh this week. We were trying to shift our belongings to our new place in Toa Payoh. We parked at the unloading bay as it granted us easy access to the lift and not with the intention of obstructing traffic. As we are moving in, there were many heavy objects and luggages that would make it extremely difficult for wife and I to carry them up and down the stairs in the HDB carpark. We hope you understand our situation. Please note that once we had transported the necessary to our flat, we had shifted the car away. We had no intention to cause any inconvenience to anyone. We seek your understanding and would appreciate that you would accede to our appeal request. Thank you for your time and Happy National Day to you.\"\n#print(predict_sentiment(test, vocab, tokenizer, model))\n#print (type(TEXT))\npre_doc=[]\ndata_pre=[]\npre_tok=clean_doc(TEXT)\npre_doc.append(pre_tok)\nprint (pre_doc)\nfor d in pre_doc:\n    data_pre.append(\" \".join(d))\n#tokens=clean_doc(test)\nMAX_SEQUENCE_LENGTH=1000\nprint (model)\nSEQUENCES = tokenizer.texts_to_matrix(data_pre,mode='tfidf')\n#max_length = max([len(s.split()) for s in TEXT])\nDATA = pad_sequences(SEQUENCES, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n#print (DATA)\nPREDICTION = model.predict(DATA,verbose=0)\n#print('result: ' + np.array(LABELS)[PREDICTION.argmax(axis=1)][0])\n\nround(PREDICTION[0,0])\n#PREDICTION", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "from repository.mlrepositoryclient import MLRepositoryClient\nfrom repository.mlrepositoryartifact import MLRepositoryArtifact\nfrom repository.mlrepository import MetaProps, MetaNames", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "wml_credentials={\n  \"url\": \"https://ibm-watson-ml.mybluemix.net\",\n  \"access_key\": \"aOvO24ysb6yucKxTq5JPExeGhdUHtus7ygRsL+AQnX+cP15qX5Esd1vHqfX212ApHxGxQ3pIogjgEOjN0TGDTcL0h32gVzPkwMbmHXNpi+FQYUqQmv73SQJrb1WXWeZv\",\n  \"username\": \"193ce3c1-5084-4e01-8054-14b1b93226e9\",\n  \"password\": \"c043d8eb-35ba-46be-a642-24db2a2418c9\",\n  \"instance_id\": \"5c6f2813-2b5d-4532-ae5a-a47c756832d1\"\n}", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "ml_repository_client = MLRepositoryClient(wml_credentials['url'])\nml_repository_client.authorize(wml_credentials['username'], wml_credentials['password'])\nprops = MetaProps({MetaNames.AUTHOR_NAME:\"Shilpa\", MetaNames.AUTHOR_EMAIL:\"jains@sg.ibm.com\"})", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "model_artifact = MLRepositoryArtifact(model, name=\"HDB Analysis\", meta_props=props)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "saved_model = ml_repository_client.models.save(model_artifact)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "saved_model.meta.available_props()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print(\"modelType: \" + saved_model.meta.prop(\"modelType\"))\nprint(\"trainingDataSchema: \" + str(saved_model.meta.prop(\"trainingDataSchema\")))\nprint(\"creationTime: \" + str(saved_model.meta.prop(\"creationTime\")))\nprint(\"modelVersionHref: \" + saved_model.meta.prop(\"modelVersionHref\"))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "loadedModelArtifact = ml_repository_client.models.get(saved_model.uid)\nprint(str(loadedModelArtifact.name))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import urllib3, requests, json\n\nheaders = urllib3.util.make_headers(basic_auth='{username}:{password}'.format(username=wml_credentials['username'], password=wml_credentials['password']))\nurl = '{}/v3/identity/token'.format(wml_credentials['url'])\nresponse = requests.get(url, headers=headers)\nmltoken = json.loads(response.text).get('token')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print(mltoken)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "endpoint_instance = wml_credentials['url'] + \"/v3/wml_instances/\" + wml_credentials['instance_id']\nheader = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken}\n\nresponse_get_instance = requests.get(endpoint_instance, headers=header)\n\nprint(response_get_instance)\nprint(response_get_instance.text)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "endpoint_published_models = json.loads(response_get_instance.text).get('entity').get('published_models').get('url')\n\nprint(endpoint_published_models)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken}\n\nresponse_get = requests.get(endpoint_published_models, headers=header)\n\nprint(response_get)\nprint(response_get.text)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}