{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "# ML Notebook for Banking Churn Model", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "## Importing Brunel and ML Libraries", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "%AddJar -magic https://brunelvis.org/jar/spark-kernel-brunel-all-2.3.jar -f", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "//import libraries\nimport org.apache.spark.{SparkConf, SparkContext, SparkFiles}\nimport org.apache.spark.sql.{SQLContext, SparkSession, Row}\nimport org.apache.spark.SparkFiles\n\nimport org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer, VectorAssembler}\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.classification.{LogisticRegression, DecisionTreeClassifier}\nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\nimport org.apache.spark.ml.{Pipeline, PipelineStage}\nimport org.apache.spark.ml.ibm.transformers.RenameColumn\n\nimport com.ibm.analytics.ngp.ingest.Sampling\nimport com.ibm.analytics.ngp.util._\nimport com.ibm.analytics.ngp.pipeline.evaluate.{Evaluator,MLProblemType}\n\nimport com.ibm.analytics.wml.{Learner, Target}\nimport com.ibm.analytics.wml.cads.CADSEstimator", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# Loading Data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "// Insert \"cust_summary_notebook_training.csv\" data set as Spark DataFrame\n\nimport org.apache.spark.sql.SQLContext\nval sqlContext = new SQLContext(sc)\n// Add data asset from file system\nval df2 = sqlContext.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"mode\", \"DROPMALFORMED\").csv(sys.env(\"DSX_PROJECT_DIR\")+\"/datasets/cust_summary_notebook_training.csv\")\ndf2.show(5)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "val churnDataRaw = df2\n\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions._ \n\nval churnData = churnDataRaw.select(\"AGE\", \"ACTIVITY\", \"EDUCATION\", \"GENDER\", \"STATE\", \"NEGTWEETS\", \"INCOME\", \"label\")\nchurnData.show(5)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "val train = 70\nval test = 15\nval validate = 15\n\n//Split the data into training data set, testing data set, and validation data set\n\nval splits = Sampling.trainingSplit(churnData, train, test, validate)\n\nval trainingDF = splits._1\nval testDF = splits._2\nval validationDF = splits._3\n\nprintln(\"Training data set\")\ntrainingDF.show(5)\n\nprintln(\"Testing data set\")\ntestDF.show(5)\n\nprintln(\"Validation data set\")\nvalidationDF.show(5)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# Building and Evaluating LR model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "//Feature definition\n\nval genderIndexer = new StringIndexer().setInputCol(\"GENDER\").setOutputCol(\"gender_code\")\nval stateIndexer = new StringIndexer().setInputCol(\"STATE\").setOutputCol(\"state_code\")\nval educationIndexer = new StringIndexer().setInputCol(\"EDUCATION\").setOutputCol(\"education_code\")\n\nval featuresAssembler = new VectorAssembler().setInputCols(Array(\"AGE\", \n                                                         \"ACTIVITY\", \n                                                         \"education_code\", \n                                                         \"NEGTWEETS\", \n                                                         \"INCOME\",\n                                                         \"gender_code\",\n                                                         \"state_code\")).setOutputCol(\"features\")", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "//Logistic Regression\n\nval lr = new LogisticRegression().setRegParam(0.01).setLabelCol(\"label\").setFeaturesCol(\"features\")", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import org.apache.spark.ml.{Pipeline, PipelineStage}\n\nval pipeline = new Pipeline().setStages(Array(genderIndexer, \n                                              stateIndexer, \n                                              educationIndexer,\n                                              featuresAssembler,\n                                              lr))\nval newModel = pipeline.fit(trainingDF)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import org.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegressionModel}\n\n// Extract the summary from the LogisticRegressionModel instance \nval lrModel = newModel.stages(4).asInstanceOf[LogisticRegressionModel]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import org.apache.spark.mllib.evaluation.MulticlassMetrics\nimport org.apache.spark.mllib.regression.LabeledPoint\n\nval testDFWithPredictions = newModel.transform(testDF)\nval testData = testDFWithPredictions.drop(\"prediction\", \"rawPrediction\", \"probability\")\nval trainingSummary = lrModel.evaluate(testData)\nval binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]\n\n// The ROC curve and area under the ROC curve on test data\nval rocOnTestData = binarySummary.roc\nprintln(s\"Area under ROC curve for the initial model: ${binarySummary.areaUnderROC}\")", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Displaying the evaluation results - ROC curve with Brunel", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "%%brunel data('rocOnTestData') x(FPR) y(TPR) line tooltip(#all) axes(x:'False Positive Rate':grid, y:'True Positive Rate':grid) title('ROC')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#  Save locally:  Save trained model to DSX Local Project", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "// DSX Local Machine Learning - Use ML client to save model.\n\nimport com.ibm.analytics.ngp.dsxML._\nimport spray.json._\n\nval ml_client=ML()\n//val modelName=\"TODO_CHANGE_TO_TEAMNAME Banking Churn Notebook Model LR\"\nval modelName=\"BankingChurnMLNotebookModelLR\"\n\n// API specification:  save(model, trainData, testData, metrics, name, description,filename, algorithmType, props: (String,String)*)\nval saveResult=ml_client.save(newModel,\n                              trainingDF,\n                              testDF,\n                              None,\n                              modelName,\n                              \"Prediction for customer to churn from business\",\n                              \"Churn ML Training Notebook Scala HDP LR.ipynb\",\n                              \"Classification\")\n\nsaveResult", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Test Locally:  Test model in DSX Local Project", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import play.api.libs.json._\nimport scalaj.http.{Http,HttpOptions}\n\nval json_map=Json.toJson(List(Json.toJson(Map(\n    \"AGE\"->Json.toJson(23),\n    \"ACTIVITY\"->Json.toJson(3),\n    \"EDUCATION\"->Json.toJson(\"Masters degree\"),\n    \"GENDER\"->Json.toJson(\"M\"),\n    \"STATE\"->Json.toJson(\"NY\"),\n    \"NEGTWEETS\"->Json.toJson(7),\n    \"INCOME\"->Json.toJson(878657)\n))))\n\nval projectName=sys.env(\"DSX_PROJECT_NAME\")\nval authToken=sys.env(\"DSX_TOKEN\")\n\nval scoringURL=s\"http://dsx-scripted-ml-python2-svc.dsxl-ml:7300/api/v1/score/unpublished/${projectName}/${modelName}\"\nprintln(scoringURL)\n\nval payload_scoring=Json.stringify(json_map)\nprintln(payload_scoring)\n\nval response_scoring=Http(scoringURL).postData(payload_scoring).header(\"Content-Type\",\"application/json\").header(\"Authorization\",authToken).option(HttpOptions.connTimeout(10000)).option(HttpOptions.readTimeout(50000)).asString\nresponse_scoring", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Developed/Updated by Alexander Petrov, Matt Walli, Anup Nair Data Science Elite Team, IBM Analytics", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark", 
            "name": "python3", 
            "language": "python3"
        }, 
        "language_info": {
            "file_extension": ".scala", 
            "version": "2.11.8", 
            "name": "scala"
        }
    }, 
    "nbformat": 4
}